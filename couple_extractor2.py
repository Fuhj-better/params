from concurrent.futures import ProcessPoolExecutor
import json
import os
import re
from pathlib import Path
from typing import List, Dict, Any, Set, Tuple
from collections import defaultdict
from dataclasses import dataclass, asdict
import networkx as nx

@dataclass
class ParamInfo:
    """å‚æ•°é…ç½®ä¿¡æ¯"""
    name: str
    type: str
    default: str
    range: str
    comment: str

@dataclass
class FileMatchResult:
    """æ–‡ä»¶åŒ¹é…ç»“æœ"""
    file_path: str
    matched_params: List[str]
    params_info: List[Dict]
    match_count: int
    contexts: List[Dict]

class StringMatcher:
    """å­—ç¬¦ä¸²åŒ¹é…ï¼šæŒ‰ç°‡æ‰«ææ–‡ä»¶ä¸­çš„å‚æ•°"""
    
    def __init__(self, driver_root: Path, clusters: Dict, params_info: Dict[str, ParamInfo]):
        self.driver_root = driver_root
        self.clusters = clusters
        self.params_info = params_info

    def remove_noise(self, content: str) -> str:
        """ç§»é™¤æ³¨é‡Šå’Œå­—ç¬¦ä¸²å­—é¢é‡ï¼ˆå‡å°‘è¯¯åŒ¹é…ï¼‰"""
        # # ç§»é™¤å•è¡Œæ³¨é‡Š //
        # content = re.sub(r'//.*?$', '', content, flags=re.MULTILINE)
        # # ç§»é™¤å¤šè¡Œæ³¨é‡Š /* */
        # content = re.sub(r'/\*.*?\*/', '', content, flags=re.DOTALL)
        # # ç§»é™¤å­—ç¬¦ä¸² "..."
        # content = re.sub(r'"[^"]*"', '""', content)
        return content
    
    def extract_context(self, content: str, match_pos: int, window: int = 200) -> Dict:
        """æå–åŒ¹é…ä½ç½®çš„ä»£ç ä¸Šä¸‹æ–‡"""
        lines = content[:match_pos].count('\n')
        start = max(0, match_pos - window)
        end = min(len(content), match_pos + window)
        
        return {
            'line_number': lines + 1,
            'snippet': content[start:end],
            'position': match_pos
        }
    
    def match_params_in_file(self, file_path: Path, cluster_params: List[str]) -> FileMatchResult:
        """åœ¨å•ä¸ªæ–‡ä»¶ä¸­åŒ¹é…æŒ‡å®šç°‡çš„å‚æ•°"""
        
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                raw_content = f.read()
        except Exception as e:
            return None
        
        # æ¸…ç†å†…å®¹
        cleaned_content = self.remove_noise(raw_content)
        
        matched_params = []
        contexts = []
        
        # åªæ‰«æè¯¥ç°‡çš„å‚æ•°
        for param_name in cluster_params:
            if param_name not in self.params_info:
                continue
            
            # ä½¿ç”¨è¯è¾¹ç•Œæ­£åˆ™ï¼ˆé¿å…éƒ¨åˆ†åŒ¹é…ï¼‰
            pattern = r'\b' + re.escape(param_name) + r'\b'
            matches = list(re.finditer(pattern, cleaned_content))
            
            if matches:
                matched_params.append(param_name)
                
                # ä¿å­˜å‰3ä¸ªåŒ¹é…ä½ç½®çš„ä¸Šä¸‹æ–‡
                for match in matches[:3]:
                    ctx = self.extract_context(cleaned_content, match.start())
                    ctx['param'] = param_name
                    contexts.append(ctx)
        
        if not matched_params:
            return None
        
        # æ„å»ºå‚æ•°è¯¦ç»†ä¿¡æ¯
        params_with_info = []
        for param in matched_params:
            param_config = self.params_info[param]
            params_with_info.append({
                'name': param_config.name,
                'type': param_config.type,
                'default': param_config.default,
                'range': param_config.range,
                'comment': param_config.comment
            })
        
        return FileMatchResult(
            file_path=str(file_path),
            matched_params=sorted(matched_params),
            params_info=params_with_info,
            match_count=len(matched_params),
            contexts=contexts
        )
    
    def scan_cluster(self, cluster_name: str, cluster_params: List[str]) -> List[Dict]:
        """æ‰«æä¸€ä¸ªå‚æ•°ç°‡"""
        
        results = []
        
        # è·å–æ‰€æœ‰æ–‡ä»¶
        all_files = [f for f in self.driver_root.rglob('*') if f.is_file()]
        print(f"å…± {len(all_files)} ä¸ªæ–‡ä»¶")
        
        for file_path in all_files:
            result = self.match_params_in_file(file_path, cluster_params)
            if result:
                results.append({
                    'file': result.file_path,
                    'matched_params': result.matched_params,
                    'params_info': result.params_info,
                    'match_count': result.match_count,
                    'contexts': result.contexts
                })
        
        # æŒ‰åŒ¹é…æ•°é‡æ’åº
        results.sort(key=lambda x: x['match_count'], reverse=True)
        
        return results
    
    def scan_all(self) -> Dict[str, List[Dict]]:
        """æŒ‰ç°‡æ‰«ææ‰€æœ‰æ–‡ä»¶"""
        
        print("="*70)
        print("ğŸ” Step 2: å­—ç¬¦ä¸²åŒ¹é…æ‰«æï¼ˆæŒ‰ç°‡ï¼‰")
        print("="*70)
        
        all_files = [f for f in self.driver_root.rglob('*') if f.is_file()]
        print(f"å…± {len(all_files)} ä¸ªæ–‡ä»¶")
        print(f"å…± {len(self.clusters)} ä¸ªå‚æ•°ç°‡\n")
        
        candidates = {}
        
        # ä¸ºæ¯ä¸ªç°‡ç‹¬ç«‹æ‰«æ
        for cluster_name, cluster_params in self.clusters.items():
            print(f"ğŸ“¦ æ‰«æç°‡: {cluster_name} ({len(cluster_params)} ä¸ªå‚æ•°)...", end=' ')
            
            cluster_results = self.scan_cluster(cluster_name, cluster_params)
            candidates[cluster_name] = cluster_results
            
            print(f"æ‰¾åˆ° {len(cluster_results)} ä¸ªæ–‡ä»¶")
        
        # ç»Ÿè®¡
        total_matches = sum(len(files) for files in candidates.values())
        print(f"\nâœ… æ€»è®¡: {total_matches} ä¸ªæ–‡ä»¶-ç°‡åŒ¹é…")
        print(f"âœ… å·²é™„åŠ å®Œæ•´å‚æ•°é…ç½®ä¿¡æ¯\n")
        
        return candidates



