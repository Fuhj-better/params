"""
æ™ºèƒ½å­—ç¬¦ä¸²åŒ¹é…é¢„ç­›é€‰
ä½œè€…: @Fuhj-better
æ—¥æœŸ: 2025-11-11
"""

import os
import re
import json
from pathlib import Path
from typing import Dict, List, Tuple
from dataclasses import dataclass
from concurrent.futures import ProcessPoolExecutor


@dataclass
class MatchResult:
    """åŒ¹é…ç»“æœæ•°æ®ç±»"""
    file_path: str
    matched_params: List[str]
    match_count: int
    code_contexts: List[Dict]
    file_type: str  # def/dut/tb
    

class SmartStringMatcher:
    """æ™ºèƒ½å­—ç¬¦ä¸²åŒ¹é…å™¨ï¼ˆé¿å…å¸¸è§è¯¯æŠ¥ï¼‰"""
    
    def __init__(self, driver_root: str):
        self.driver_root = Path(driver_root)
        
    def remove_noise(self, content: str) -> str:
        """ç§»é™¤æ³¨é‡Šå’Œå­—ç¬¦ä¸²å­—é¢é‡ï¼ˆå‡å°‘è¯¯åŒ¹é…ï¼‰"""
        # ç§»é™¤å•è¡Œæ³¨é‡Š //
        content = re.sub(r'//.*?$', '', content, flags=re.MULTILINE)
        # ç§»é™¤å¤šè¡Œæ³¨é‡Š /* */
        content = re.sub(r'/\*.*?\*/', '', content, flags=re.DOTALL)
        # ç§»é™¤å­—ç¬¦ä¸² "..."
        content = re.sub(r'"[^"]*"', '""', content)
        return content
    
    def extract_context(self, content: str, match_pos: int, 
                       window: int = 200) -> Dict:
        """æå–åŒ¹é…ä½ç½®çš„ä»£ç ä¸Šä¸‹æ–‡"""
        lines = content[:match_pos].count('\n')
        start = max(0, match_pos - window)
        end = min(len(content), match_pos + window)
        
        return {
            'line_number': lines + 1,
            'snippet': content[start:end],
            'position': match_pos
        }
    
    def match_params_in_file(self, file_path: Path, 
                            params: List[str]) -> MatchResult:
        """åœ¨å•ä¸ªæ–‡ä»¶ä¸­åŒ¹é…å‚æ•°"""
        
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                raw_content = f.read()
        except Exception as e:
            print(f"âš ï¸  æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {e}")
            return None
        
        # æ¸…ç†å†…å®¹
        cleaned_content = self.remove_noise(raw_content)
        
        matched_params = []
        contexts = []
        total_matches = 0
        
        for param in params:
            # ä½¿ç”¨è¯è¾¹ç•Œæ­£åˆ™ï¼ˆé¿å…éƒ¨åˆ†åŒ¹é…ï¼Œå¦‚ CLK_EN åŒ¹é…åˆ° CLK_ENABLEï¼‰
            pattern = r'\b' + re.escape(param) + r'\b'
            matches = list(re.finditer(pattern, cleaned_content))
            
            if matches:
                matched_params.append(param)
                total_matches += len(matches)
                
                # ä¿å­˜å‰3ä¸ªåŒ¹é…ä½ç½®çš„ä¸Šä¸‹æ–‡
                for match in matches[:3]:
                    ctx = self.extract_context(cleaned_content, match.start())
                    ctx['param'] = param
                    contexts.append(ctx)
        
        # åˆ¤æ–­æ–‡ä»¶ç±»å‹
        file_type = self.classify_file(file_path)
        
        return MatchResult(
            file_path=str(file_path),
            matched_params=matched_params,
            match_count=total_matches,
            code_contexts=contexts,
            file_type=file_type
        )
    
    def classify_file(self, file_path: Path) -> str:
        """åˆ†ç±»æ–‡ä»¶ç±»å‹"""
        path_str = str(file_path)
        if '/def/' in path_str or '\\def\\' in path_str:
            return 'def'
        elif '/dut/' in path_str or '\\dut\\' in path_str:
            return 'dut'
        elif '/tb/' in path_str or '\\tb\\' in path_str:
            return 'tb'
        return 'unknown'
    
    def scan_cluster(self, cluster_name: str, 
                    params: List[str]) -> List[MatchResult]:
        """æ‰«æä¸€ä¸ªå‚æ•°ç°‡"""
        
        results = []
        
        # éå†driverä¸‹æ‰€æœ‰.sv/.væ–‡ä»¶
        for sv_file in self.driver_root.rglob('*.sv'):
            result = self.match_params_in_file(sv_file, params)
            if result and result.match_count > 0:
                results.append(result)
        
        for v_file in self.driver_root.rglob('*.v'):
            result = self.match_params_in_file(v_file, params)
            if result and result.match_count > 0:
                results.append(result)
        
        # æŒ‰åŒ¹é…æ•°é‡æ’åº
        results.sort(key=lambda x: x.match_count, reverse=True)
        
        return results
    
    def parallel_scan(self, clusters: Dict[str, List[str]], 
                     max_workers: int = 4) -> Dict:
        """å¹¶è¡Œæ‰«ææ‰€æœ‰ç°‡"""
        
        all_results = {}
        
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(self.scan_cluster, name, params): name
                for name, params in clusters.items()
            }
            
            for future in futures:
                cluster_name = futures[future]
                try:
                    results = future.result()
                    all_results[cluster_name] = [
                        {
                            'file': r.file_path,
                            'matched_params': r.matched_params,
                            'match_count': r.match_count,
                            'file_type': r.file_type,
                            'contexts': r.code_contexts[:3]  # æœ€å¤šä¿ç•™3ä¸ªä¸Šä¸‹æ–‡
                        }
                        for r in results
                    ]
                    print(f"âœ… {cluster_name}: æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡ä»¶")
                except Exception as e:
                    print(f"âŒ {cluster_name} æ‰«æå¤±è´¥: {e}")
        
        return all_results


def main():
    """ä¸»å‡½æ•°"""
    
    # 1. åŠ è½½å‚æ•°ç°‡
    with open('clusters.json', 'r') as f:
        clusters = json.load(f)
    
    print(f"ğŸ“¦ åŠ è½½äº† {len(clusters)} ä¸ªå‚æ•°ç°‡")
    
    # 2. æ‰§è¡Œå­—ç¬¦ä¸²åŒ¹é…
    matcher = SmartStringMatcher(driver_root='driver/')
    
    print("\nğŸ” å¼€å§‹å­—ç¬¦ä¸²åŒ¹é…æ‰«æ...\n")
    candidates = matcher.parallel_scan(clusters, max_workers=8)
    
    # 3. ä¿å­˜ç»“æœ
    with open('candidates.json', 'w') as f:
        json.dump(candidates, f, indent=2, ensure_ascii=False)
    
    # 4. ç»Ÿè®¡è¾“å‡º
    total_candidates = sum(len(files) for files in candidates.values())
    print(f"\nğŸ“Š æ‰«æå®Œæˆï¼")
    print(f"   - æ€»å€™é€‰æ–‡ä»¶å¯¹: {total_candidates}")
    print(f"   - å·²ä¿å­˜åˆ°: candidates.json")
    
    # 5. æ˜¾ç¤ºæ¯ä¸ªç°‡çš„topæ–‡ä»¶
    print("\nğŸ“‹ æ¯ä¸ªç°‡çš„é«˜ç›¸å…³æ–‡ä»¶ï¼š")
    for cluster, files in candidates.items():
        if files:
            top_file = files[0]
            print(f"   {cluster}: {Path(top_file['file']).name} "
                  f"({top_file['match_count']}æ¬¡åŒ¹é…)")


if __name__ == '__main__':
    main()