"""
å®Œæ•´çš„å‚æ•°è€¦åˆåˆ†æç³»ç»Ÿ
åŸºäºå·²æ„å»ºçš„ä»£ç ä¾èµ–å…³ç³» (dependency_analysis.json)
ä½œè€…: @Fuhj-better
æ—¥æœŸ: 2025-11-11
"""

import json
import os
import re
from pathlib import Path
from typing import List, Dict, Any, Set, Tuple
from collections import defaultdict
from dataclasses import dataclass, asdict
import networkx as nx


# =============================================================================
# æ•°æ®ç±»å®šä¹‰
# =============================================================================

@dataclass
class ParamInfo:
    """å‚æ•°é…ç½®ä¿¡æ¯"""
    name: str
    type: str
    default: str
    range: str
    comment: str

@dataclass
class FileMatchResult:
    """æ–‡ä»¶åŒ¹é…ç»“æœ"""
    file_path: str
    matched_params: List[str]
    params_info: List[Dict]
    match_count: int
    contexts: List[Dict]


# =============================================================================
# Step 1: åŠ è½½ä¾èµ–ä¿¡æ¯å’Œå‚æ•°é…ç½®
# =============================================================================

class DependencyLoader:
    """åŠ è½½ä»£ç ä¾èµ–ä¿¡æ¯å’Œå‚æ•°é…ç½®"""
    
    def __init__(self, dependency_json: Path, clusters_json: Path, params_file: Path):
        self.dependency_json = dependency_json
        self.clusters_json = clusters_json
        self.params_file = params_file
        
        self.dependency_data = None
        self.clusters = None
        self.params_info = {}  # å­˜å‚¨å®Œæ•´çš„å‚æ•°ä¿¡æ¯

    def parse_param_line(self, line: str) -> ParamInfo:
        """
        è§£æå‚æ•°é…ç½®è¡Œ
        æ ¼å¼: PARAM_NAME(type) value (range) [# comment]
        ä¾‹å¦‚: FDAE_WIDTH(int) 32 (1-1024) # Data width
        """
        line = line.strip()
        if not line or line.startswith('#'):
            return None
        
        # æå–æ³¨é‡Š
        comment = ''
        if '#' in line:
            line, comment = line.split('#', 1)
            comment = comment.strip()
            line = line.strip()
        
        # åŒ¹é…æ ¼å¼: NAME(type) value (range)
        pattern = r'(\S+)\s*\((\w+)\)\s+(\S+)(?:\s+\(([^)]+)\))?'
        match = re.match(pattern, line)
        
        if not match:
            return None
        
        return ParamInfo(
            name=match.group(1),
            type=match.group(2),
            default=match.group(3),
            range=match.group(4) if match.group(4) else '',
            comment=comment
        )
    
    def load(self):
        """åŠ è½½æ‰€æœ‰å¿…è¦æ•°æ®"""
        
        print("="*70)
        print("ğŸ“‚ Step 1: åŠ è½½æ•°æ®")
        print("="*70)
        
        # 1. åŠ è½½ä¾èµ–åˆ†æç»“æœ
        if self.dependency_json.exists():
            with open(self.dependency_json, 'r', encoding='utf-8') as f:
                self.dependency_data = json.load(f)
            
            summary = self.dependency_data.get('dependency_analysis', {}).get('summary', {})
            print(f"âœ… åŠ è½½ä»£ç ä¾èµ–ä¿¡æ¯:")
            print(f"   - æ–‡ä»¶æ•°: {summary.get('total_files', 0)}")
            print(f"   - ä¾èµ–å…³ç³»: {summary.get('total_dependencies', 0)}")
            print(f"   - æ¨¡å—å®ä¾‹åŒ–: {summary.get('module_dependencies', 0)}")
        else:
            print(f"âš ï¸  æœªæ‰¾åˆ° {self.dependency_json}")
            self.dependency_data = {}
        
        # 2. åŠ è½½å‚æ•°ç°‡
        if self.clusters_json.exists():
            with open(self.clusters_json, 'r', encoding='utf-8') as f:
                self.clusters = json.load(f)
            print(f"âœ… åŠ è½½å‚æ•°ç°‡: {len(self.clusters)} ä¸ªç°‡")
        else:
            print(f"âš ï¸  æœªæ‰¾åˆ° {self.clusters_json}")
            self.clusters = {}
        
        # 3. åŠ è½½å¹¶è§£æå‚æ•°é…ç½®æ–‡ä»¶
        if self.params_file.exists():
            with open(self.params_file, 'r', encoding='utf-8') as f:
                for line in f:
                    param_info = self.parse_param_line(line)
                    if param_info:
                        self.params_info[param_info.name] = param_info
            
            print(f"âœ… åŠ è½½å‚æ•°é…ç½®: {len(self.params_info)} ä¸ªå‚æ•°")
            
            # æ‰“å°ç¤ºä¾‹
            if self.params_info:
                first_param = next(iter(self.params_info.values()))
                print(f"   ç¤ºä¾‹å‚æ•°: {first_param.name}")
                print(f"     - ç±»å‹: {first_param.type}")
                print(f"     - é»˜è®¤å€¼: {first_param.default}")
                print(f"     - èŒƒå›´: {first_param.range}")
                if first_param.comment:
                    print(f"     - è¯´æ˜: {first_param.comment}")
        else:
            print(f"âš ï¸  æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ {self.params_file}")
        
        print()
        return self


# =============================================================================
# Step 2: å­—ç¬¦ä¸²åŒ¹é… - æ‰¾å‡ºå“ªäº›æ–‡ä»¶ä½¿ç”¨äº†å“ªäº›å‚æ•°
# =============================================================================

class StringMatcher:
    """å­—ç¬¦ä¸²åŒ¹é…ï¼šæŒ‰ç°‡æ‰«ææ–‡ä»¶ä¸­çš„å‚æ•°"""
    
    def __init__(self, driver_root: Path, clusters: Dict, params_info: Dict[str, ParamInfo]):
        self.driver_root = driver_root
        self.clusters = clusters
        self.params_info = params_info

    def remove_noise(self, content: str) -> str:
        """ç§»é™¤æ³¨é‡Šå’Œå­—ç¬¦ä¸²å­—é¢é‡ï¼ˆå‡å°‘è¯¯åŒ¹é…ï¼‰"""
        # ç§»é™¤å•è¡Œæ³¨é‡Š //
        content = re.sub(r'//.*?$', '', content, flags=re.MULTILINE)
        # ç§»é™¤å¤šè¡Œæ³¨é‡Š /* */
        content = re.sub(r'/\*.*?\*/', '', content, flags=re.DOTALL)
        # ç§»é™¤å­—ç¬¦ä¸² "..."
        content = re.sub(r'"[^"]*"', '""', content)
        return content
    
    def extract_context(self, content: str, match_pos: int, window: int = 200) -> Dict:
        """æå–åŒ¹é…ä½ç½®çš„ä»£ç ä¸Šä¸‹æ–‡"""
        lines = content[:match_pos].count('\n')
        start = max(0, match_pos - window)
        end = min(len(content), match_pos + window)
        
        return {
            'line_number': lines + 1,
            'snippet': content[start:end],
            'position': match_pos
        }
    
    def match_params_in_file(self, file_path: Path, cluster_params: List[str]) -> FileMatchResult:
        """åœ¨å•ä¸ªæ–‡ä»¶ä¸­åŒ¹é…æŒ‡å®šç°‡çš„å‚æ•°"""
        
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                raw_content = f.read()
        except Exception as e:
            return None
        
        # æ¸…ç†å†…å®¹
        cleaned_content = self.remove_noise(raw_content)
        
        matched_params = []
        contexts = []
        
        # åªæ‰«æè¯¥ç°‡çš„å‚æ•°
        for param_name in cluster_params:
            if param_name not in self.params_info:
                continue
            
            # ä½¿ç”¨è¯è¾¹ç•Œæ­£åˆ™ï¼ˆé¿å…éƒ¨åˆ†åŒ¹é…ï¼‰
            pattern = r'\b' + re.escape(param_name) + r'\b'
            matches = list(re.finditer(pattern, cleaned_content))
            
            if matches:
                matched_params.append(param_name)
                
                # ä¿å­˜å‰3ä¸ªåŒ¹é…ä½ç½®çš„ä¸Šä¸‹æ–‡
                for match in matches[:3]:
                    ctx = self.extract_context(cleaned_content, match.start())
                    ctx['param'] = param_name
                    contexts.append(ctx)
        
        if not matched_params:
            return None
        
        # æ„å»ºå‚æ•°è¯¦ç»†ä¿¡æ¯
        params_with_info = []
        for param in matched_params:
            param_config = self.params_info[param]
            params_with_info.append({
                'name': param_config.name,
                'type': param_config.type,
                'default': param_config.default,
                'range': param_config.range,
                'comment': param_config.comment
            })
        
        return FileMatchResult(
            file_path=str(file_path),
            matched_params=sorted(matched_params),
            params_info=params_with_info,
            match_count=len(matched_params),
            contexts=contexts
        )
    
    def scan_cluster(self, cluster_name: str, cluster_params: List[str]) -> List[Dict]:
        """æ‰«æä¸€ä¸ªå‚æ•°ç°‡"""
        
        results = []
        
        # è·å–æ‰€æœ‰æ–‡ä»¶
        all_files = [f for f in self.driver_root.rglob('*') if f.is_file()]
        print(f"å…± {len(all_files)} ä¸ªæ–‡ä»¶")
        
        for file_path in all_files:
            result = self.match_params_in_file(file_path, cluster_params)
            if result:
                results.append({
                    'file': result.file_path,
                    'matched_params': result.matched_params,
                    'params_info': result.params_info,
                    'match_count': result.match_count,
                    'contexts': result.contexts
                })
        
        # æŒ‰åŒ¹é…æ•°é‡æ’åº
        results.sort(key=lambda x: x['match_count'], reverse=True)
        
        return results
    
    def scan_all(self) -> Dict[str, List[Dict]]:
        """æŒ‰ç°‡æ‰«ææ‰€æœ‰æ–‡ä»¶"""
        
        print("="*70)
        print("ğŸ” Step 2: å­—ç¬¦ä¸²åŒ¹é…æ‰«æï¼ˆæŒ‰ç°‡ï¼‰")
        print("="*70)
        
        all_files = [f for f in self.driver_root.rglob('*') if f.is_file()]
        print(f"å…± {len(all_files)} ä¸ªæ–‡ä»¶")
        print(f"å…± {len(self.clusters)} ä¸ªå‚æ•°ç°‡\n")
        
        candidates = {}
        
        # ä¸ºæ¯ä¸ªç°‡ç‹¬ç«‹æ‰«æ
        for cluster_name, cluster_params in self.clusters.items():
            print(f"ğŸ“¦ æ‰«æç°‡: {cluster_name} ({len(cluster_params)} ä¸ªå‚æ•°)...", end=' ')
            
            cluster_results = self.scan_cluster(cluster_name, cluster_params)
            candidates[cluster_name] = cluster_results
            
            print(f"æ‰¾åˆ° {len(cluster_results)} ä¸ªæ–‡ä»¶")
        
        # ç»Ÿè®¡
        total_matches = sum(len(files) for files in candidates.values())
        print(f"\nâœ… æ€»è®¡: {total_matches} ä¸ªæ–‡ä»¶-ç°‡åŒ¹é…")
        print(f"âœ… å·²é™„åŠ å®Œæ•´å‚æ•°é…ç½®ä¿¡æ¯\n")
        
        return candidates


# =============================================================================
# Step 3: åŸºäºä¾èµ–å…³ç³»æ„å»ºç°‡å¯¹ï¼ˆæ”¹é€ ç‰ˆï¼‰
# =============================================================================

class ClusterPairBuilder:
    """æ„å»ºç°‡å¯¹åˆ†æä»»åŠ¡(æ›´ç²¾ç»†çš„ç²’åº¦)"""
    
    def __init__(self, dependency_data: Dict, candidates: Dict, clusters_def: Dict):
        self.dependency_data = dependency_data
        self.candidates = candidates
        self.clusters_def = clusters_def
    
    def build_pairs(self) -> List[Dict]:
        """ç”Ÿæˆä¸¤ä¸¤ç°‡å¯¹çš„åˆ†æä»»åŠ¡(å»é‡ç‰ˆ + è¿‡æ»¤æœªä½¿ç”¨çš„ç°‡)"""
        
        print("="*70)
        print("ğŸ”— Step 3: æ„å»ºç°‡å¯¹åˆ†æä»»åŠ¡ï¼ˆä»¥ç°‡å¯¹ä¸ºä¸­å¿ƒï¼‰")
        print("="*70)
        
        # é¦–å…ˆæ„å»ºæ–‡ä»¶->å‚æ•°æ˜ å°„ï¼ˆæŒ‰ç°‡åˆ†ç»„ï¼‰
        file_to_params = self._build_file_param_mapping()
        
        # è¯†åˆ«å®é™…è¢«ä½¿ç”¨çš„ç°‡
        used_clusters = set()
        for file_data in file_to_params.values():
            used_clusters.update(file_data['clusters'])
        
        # ç»Ÿè®¡æœªä½¿ç”¨çš„ç°‡
        all_clusters = set(self.clusters_def.keys())
        unused_clusters = all_clusters - used_clusters
        
        print(f"ğŸ“Š å‚æ•°ç°‡ç»Ÿè®¡:")
        print(f"   - å®šä¹‰çš„ç°‡æ€»æ•°: {len(all_clusters)}")
        print(f"   - å®é™…ä½¿ç”¨çš„ç°‡: {len(used_clusters)}")
        print(f"   - æœªä½¿ç”¨çš„ç°‡: {len(unused_clusters)}")
        
        if unused_clusters:
            print(f"\nâš ï¸  ä»¥ä¸‹ç°‡æœªåœ¨ä»£ç ä¸­ä½¿ç”¨ï¼Œå°†è¢«è·³è¿‡:")
            for cluster in sorted(unused_clusters):
                print(f"      - {cluster}")
        print()
        
        # åªæšä¸¾è¢«ä½¿ç”¨çš„ç°‡å¯¹
        from itertools import combinations
        used_cluster_list = sorted(list(used_clusters))
        all_possible_pairs = list(combinations(used_cluster_list, 2))
        
        print(f"ğŸ“Š åŸºäº {len(used_clusters)} ä¸ªä½¿ç”¨ä¸­çš„ç°‡:")
        print(f"   - ç†è®ºç°‡å¯¹æ•°: {len(all_possible_pairs)}\n")
        
        # ä½¿ç”¨å­—å…¸å­˜å‚¨ç°‡å¯¹ï¼Œé”®ä¸ºè§„èŒƒåŒ–çš„ç°‡å¯¹æ ‡è¯†
        cluster_pair_dict = {}
        
        # ä¸ºæ¯ä¸ªç°‡å¯¹æ”¶é›†ä»£ç ä¸Šä¸‹æ–‡
        for cluster1, cluster2 in all_possible_pairs:
            contexts = self._collect_contexts_for_cluster_pair(
                cluster1, cluster2, file_to_params
            )
            
            if contexts:  # åªä¿ç•™æœ‰ä»£ç ä¸Šä¸‹æ–‡çš„ç°‡å¯¹
                pair_key = self._make_cluster_pair_key(cluster1, cluster2)
                cluster_pair_dict[pair_key] = {
                    'cluster_pair': (cluster1, cluster2),
                    'contexts': contexts,
                    'context_count': len(contexts),
                    'has_intra_file': any(c['type'] == 'INTRA_FILE' for c in contexts),
                    'has_inter_file': any(c['type'] == 'INTER_FILE' for c in contexts)
                }
        
        cluster_pairs = list(cluster_pair_dict.values())
        
        # ç»Ÿè®¡
        print(f"âœ… æ„å»ºäº† {len(cluster_pairs)} ä¸ªæœ‰ä»£ç ä¸Šä¸‹æ–‡çš„ç°‡å¯¹")
        print(f"   - ä»…å•æ–‡ä»¶å†…å…±ç°: {sum(1 for p in cluster_pairs if p['has_intra_file'] and not p['has_inter_file'])}")
        print(f"   - ä»…è·¨æ–‡ä»¶ä¾èµ–: {sum(1 for p in cluster_pairs if p['has_inter_file'] and not p['has_intra_file'])}")
        print(f"   - ä¸¤è€…éƒ½æœ‰: {sum(1 for p in cluster_pairs if p['has_intra_file'] and p['has_inter_file'])}")
        print(f"   - æ— ä»£ç å…³è”çš„ç°‡å¯¹: {len(all_possible_pairs) - len(cluster_pairs)} (å·²è¿‡æ»¤)\n")
        
        return cluster_pairs
    
    def _build_file_param_mapping(self) -> Dict:
        """æ„å»ºæ–‡ä»¶->å‚æ•°æ˜ å°„ï¼ˆæŒ‰ç°‡åˆ†ç»„ï¼‰"""
        file_to_params = {}
        
        for cluster_name, files in self.candidates.items():
            # è·³è¿‡æ²¡æœ‰åŒ¹é…æ–‡ä»¶çš„ç°‡
            if not files:
                continue
                
            for f in files:
                fp = f['file']
                if fp not in file_to_params:
                    file_to_params[fp] = {
                        'clusters': set(),
                        'params_by_cluster': {}
                    }
                
                file_to_params[fp]['clusters'].add(cluster_name)
                file_to_params[fp]['params_by_cluster'][cluster_name] = {
                    'params': f['matched_params'],
                    'params_info': f['params_info'],
                    'contexts': f.get('contexts', [])
                }
        
        return file_to_params
    
    def _collect_contexts_for_cluster_pair(self, 
                                           cluster1: str, 
                                           cluster2: str,
                                           file_to_params: Dict) -> List[Dict]:
        """ä¸ºæŒ‡å®šç°‡å¯¹æ”¶é›†æ‰€æœ‰ä»£ç ä¸Šä¸‹æ–‡"""
        contexts = []
        
        # 1. å•æ–‡ä»¶å†…å…±ç°
        for file_path, file_data in file_to_params.items():
            file_clusters = file_data['clusters']
            
            if cluster1 in file_clusters and cluster2 in file_clusters:
                contexts.append({
                    'type': 'INTRA_FILE',
                    'file': file_path,
                    'cluster1': cluster1,
                    'cluster2': cluster2,
                    'cluster1_params': file_data['params_by_cluster'][cluster1],
                    'cluster2_params': file_data['params_by_cluster'][cluster2]
                })
        
        # 2. è·¨æ–‡ä»¶ä¾èµ–
        module_deps = (self.dependency_data
                      .get('dependency_analysis', {})
                      .get('dependency_relationships', {})
                      .get('module_dependencies', []))
        
        for dep in module_deps:
            caller = dep.get('source_path')
            callee = dep.get('target_path')
            
            caller_info = file_to_params.get(caller)
            callee_info = file_to_params.get(callee)
            
            if not (caller_info and callee_info):
                continue
            
            caller_clusters = caller_info['clusters']
            callee_clusters = callee_info['clusters']
            
            # æƒ…å†µ1: calleræœ‰cluster1, calleeæœ‰cluster2
            if cluster1 in caller_clusters and cluster2 in callee_clusters:
                contexts.append({
                    'type': 'INTER_FILE',
                    'caller_file': caller,
                    'callee_file': callee,
                    'caller_cluster': cluster1,
                    'callee_cluster': cluster2,
                    'caller_params': caller_info['params_by_cluster'][cluster1],
                    'callee_params': callee_info['params_by_cluster'][cluster2],
                    'module': dep.get('module_type'),
                    'instance': dep.get('instance_name'),
                    'instantiation_code': dep.get('description', ''),
                    'direction': f'{cluster1}â†’{cluster2}'
                })
            
            # æƒ…å†µ2: calleræœ‰cluster2, calleeæœ‰cluster1ï¼ˆåå‘ï¼‰
            if cluster2 in caller_clusters and cluster1 in callee_clusters:
                contexts.append({
                    'type': 'INTER_FILE',
                    'caller_file': caller,
                    'callee_file': callee,
                    'caller_cluster': cluster2,
                    'callee_cluster': cluster1,
                    'caller_params': caller_info['params_by_cluster'][cluster2],
                    'callee_params': callee_info['params_by_cluster'][cluster1],
                    'module': dep.get('module_type'),
                    'instance': dep.get('instance_name'),
                    'instantiation_code': dep.get('description', ''),
                    'direction': f'{cluster2}â†’{cluster1}'
                })
        
        return contexts
    
    def _make_cluster_pair_key(self, cluster1: str, cluster2: str) -> str:
        """ç”Ÿæˆç°‡å¯¹çš„å”¯ä¸€é”®ï¼ˆæ— åºï¼‰"""
        c1, c2 = sorted([cluster1, cluster2])
        return f"{c1}â†”{c2}"
# =============================================================================
# Step 3: åŸºäºä¾èµ–å…³ç³»æ„å»ºæ–‡ä»¶å¯¹
# =============================================================================

class FilePairBuilder:
    """åŸºäºä»£ç ä¾èµ–å…³ç³»æ„å»ºæ–‡ä»¶å¯¹"""
    
    def __init__(self, dependency_data: Dict, candidates: Dict):
        self.dependency_data = dependency_data
        self.candidates = candidates
        
    def build_pairs(self) -> List[Dict]:
        """ä» dependency_analysis.json æå–æ–‡ä»¶å¯¹ï¼Œæ”¯æŒå•ç«¯å¤šç°‡å’ŒåŒç«¯åˆ†æ"""
        
        print("="*70)
        print("ğŸ”— Step 3: æ„å»ºæ–‡ä»¶è°ƒç”¨å¯¹ï¼ˆåŸºäºä¾èµ–åˆ†æï¼‰")
        print("="*70)
        
        module_deps = (self.dependency_data
                      .get('dependency_analysis', {})
                      .get('dependency_relationships', {})
                      .get('module_dependencies', []))
        
        file_pairs = []
        
        # æ„å»ºæ–‡ä»¶->å‚æ•°æ˜ å°„ï¼ˆåŒ…å«å®Œæ•´ä¿¡æ¯ï¼‰
        file_to_params = {}
        for cluster_name, files in self.candidates.items():
            for f in files:
                fp = f['file']
                if fp not in file_to_params:
                    file_to_params[fp] = {
                        'params': set(),
                        'params_info': [],
                        'clusters': set(),
                        'contexts': []
                    }
                file_to_params[fp]['params'].update(f['matched_params'])
                file_to_params[fp]['params_info'].extend(f['params_info'])
                file_to_params[fp]['clusters'].add(cluster_name)
                file_to_params[fp]['contexts'].extend(f['contexts'])
        
        # æ„å»ºæ–‡ä»¶å¯¹
        for dep in module_deps:
            caller = dep.get('source_path')
            callee = dep.get('target_path')
            
            caller_info = file_to_params.get(caller)
            callee_info = file_to_params.get(callee)
            
            # æƒ…å†µ1: åŒç«¯éƒ½æœ‰å‚æ•° - åˆ†æè·¨æ–‡ä»¶è€¦åˆ
            if caller_info and callee_info:
                file_pairs.append({
                    'type': 'INTER_FILE',  # è·¨æ–‡ä»¶åˆ†æ
                    'caller_file': caller,
                    'callee_file': callee,
                    'module': dep.get('module_type'),
                    'instance': dep.get('instance_name'),
                    'instantiation_code': dep.get('description', ''),
                    'caller_params': sorted(list(caller_info['params'])),
                    'caller_params_info': caller_info['params_info'],
                    'callee_params': sorted(list(callee_info['params'])),
                    'callee_params_info': callee_info['params_info'],
                    'caller_clusters': sorted(list(caller_info['clusters'])),
                    'callee_clusters': sorted(list(callee_info['clusters']))
                })
            
            # æƒ…å†µ2: åªæœ‰calleræœ‰å‚æ•°ï¼Œä¸”å±äºå¤šä¸ªç°‡ - åˆ†æç°‡é—´è€¦åˆ
            elif caller_info and not callee_info and len(caller_info['clusters']) > 1:
                file_pairs.append({
                    'type': 'INTRA_FILE_MULTI_CLUSTER',  # å•æ–‡ä»¶å¤šç°‡åˆ†æ
                    'file': caller,
                    'role': 'caller',
                    'module': dep.get('module_type'),
                    'instance': dep.get('instance_name'),
                    'instantiation_code': dep.get('description', ''),
                    'params': sorted(list(caller_info['params'])),
                    'params_info': caller_info['params_info'],
                    'clusters': sorted(list(caller_info['clusters'])),
                    'other_file': callee  # è®°å½•å…³è”æ–‡ä»¶
                })
            
            # æƒ…å†µ3: åªæœ‰calleeæœ‰å‚æ•°ï¼Œä¸”å±äºå¤šä¸ªç°‡ - åˆ†æç°‡é—´è€¦åˆ
            elif callee_info and not caller_info and len(callee_info['clusters']) > 1:
                file_pairs.append({
                    'type': 'INTRA_FILE_MULTI_CLUSTER',  # å•æ–‡ä»¶å¤šç°‡åˆ†æ
                    'file': callee,
                    'role': 'callee',
                    'module': dep.get('module_type'),
                    'instance': dep.get('instance_name'),
                    'instantiation_code': dep.get('description', ''),
                    'params': sorted(list(callee_info['params'])),
                    'params_info': callee_info['params_info'],
                    'clusters': sorted(list(callee_info['clusters'])),
                    'other_file': caller  # è®°å½•å…³è”æ–‡ä»¶
                })
        
        # ç»Ÿè®¡
        inter_file_count = sum(1 for p in file_pairs if p['type'] == 'INTER_FILE')
        intra_file_count = sum(1 for p in file_pairs if p['type'] == 'INTRA_FILE_MULTI_CLUSTER')
        
        print(f"âœ… æ„å»ºäº† {len(file_pairs)} ä¸ªåˆ†æä»»åŠ¡:")
        print(f"   - è·¨æ–‡ä»¶åˆ†æ (ä¸¤ç«¯éƒ½æœ‰å‚æ•°): {inter_file_count}")
        print(f"   - å•æ–‡ä»¶å¤šç°‡åˆ†æ (ä¸€ç«¯å¤šç°‡): {intra_file_count}")
        print(f"âœ… æ¯ä¸ªä»»åŠ¡éƒ½åŒ…å«å®Œæ•´çš„å‚æ•°é…ç½®ä¿¡æ¯\n")
        
        return file_pairs


# =============================================================================
# Step 4: LLM åˆ†ææ–‡ä»¶å¯¹
# =============================================================================

# =============================================================================
# Step 4: LLM Analysis - Cluster Pair Centric Approach
# =============================================================================

class LLMCouplingAnalyzer:
    """Analyze parameter coupling between cluster pairs using LLM"""
    
    def __init__(self, cluster_pairs: List[Dict], clusters_def: Dict):
        """
        Args:
            cluster_pairs: List of cluster pair tasks from ClusterPairBuilder
            clusters_def: Original cluster definitions (cluster_name -> param_list)
        """
        self.cluster_pairs = cluster_pairs
        self.clusters_def = clusters_def
    
    def _format_params_info(self, params_info: List[Dict]) -> str:
        """Format parameter information into readable text"""
        lines = []
        for p in params_info:
            line = f"- **{p['name']}** ({p['type']})"
            if p.get('default'):
                line += f" = {p['default']}"
            if p.get('range'):
                line += f" [{p['range']}]"
            if p.get('comment'):
                line += f"  // {p['comment']}"
            lines.append(line)
        return '\n'.join(lines) if lines else "  (No parameters)"
    
    def generate_prompt(self, pair_task: Dict) -> str:
        """Generate LLM prompt for analyzing a cluster pair
        
        This method aggregates all code contexts where the cluster pair appears
        """
        cluster1, cluster2 = pair_task['cluster_pair']
        contexts = pair_task['contexts']
        
        # Get parameter definitions from the first context
        first_ctx = contexts[0]
        if first_ctx['type'] == 'INTRA_FILE':
            cluster1_params = first_ctx['cluster1_params']['params_info']
            cluster2_params = first_ctx['cluster2_params']['params_info']
        else:  # INTER_FILE
            cluster1_params = first_ctx['caller_params']['params_info']
            cluster2_params = first_ctx['callee_params']['params_info']
        
        prompt = f"""# Hardware Parameter Cluster Coupling Analysis

## Objective
Analyze the coupling relationships between two parameter clusters:
- **Cluster 1**: {cluster1}
- **Cluster 2**: {cluster2}

---

## Cluster 1 Parameter Definitions: {cluster1}
{self._format_params_info(cluster1_params)}

---

## Cluster 2 Parameter Definitions: {cluster2}
{self._format_params_info(cluster2_params)}

---

## Code Contexts

This cluster pair appears in **{len(contexts)}** code context(s):

"""
        
        # List all contexts
        for i, ctx in enumerate(contexts, 1):
            if ctx['type'] == 'INTRA_FILE':
                prompt += f"""
### Context {i}: Intra-File Co-occurrence
- **File**: `{Path(ctx['file']).name}`
- **Description**: This file uses parameters from both clusters
- **{cluster1} parameter count**: {len(ctx['cluster1_params']['params'])}
- **{cluster2} parameter count**: {len(ctx['cluster2_params']['params'])}
- **Sample {cluster1} params**: {', '.join(ctx['cluster1_params']['params'][:5])}{'...' if len(ctx['cluster1_params']['params']) > 5 else ''}
- **Sample {cluster2} params**: {', '.join(ctx['cluster2_params']['params'][:5])}{'...' if len(ctx['cluster2_params']['params']) > 5 else ''}
"""
            else:  # INTER_FILE
                prompt += f"""
### Context {i}: Inter-File Dependency
- **Calling relationship**: `{Path(ctx['caller_file']).name}` â†’ `{Path(ctx['callee_file']).name}`
- **Cluster direction**: {ctx['direction']}
- **Module type**: {ctx['module']}
- **Instance name**: {ctx['instance']}
- **Instantiation code snippet**: 
  ```verilog
  {ctx['instantiation_code'][:300]}   
  """
        prompt += """
## Analysis Guidelines
1. Common Hardware Parameter Coupling Patterns
A. DIRECT_PASS (Direct Parameter Passing)

Caller passes parameter value to callee through instantiation
Example: top_width â†’ fifo_width (via #(.WIDTH(top_width)))
B. DERIVATION (Derived Calculation)

One parameter is mathematically derived from another
Example: addr_width = log2(depth)
C. CONSTRAINT (Constraint Relationship)

Parameters must satisfy inequalities or equations
Example: input_width <= output_width (avoid data truncation)
Example: cache_line_size % bus_width == 0 (alignment requirement)
D. CONDITIONAL (Conditional Dependency)

One parameter's value determines another's validity or value
Example: if enable_ecc==1 then ecc_width=8 else ecc_width=0
E. RESOURCE (Resource Constraint)

Multiple parameters share resource limitations
Example: num_channels * channel_width <= total_bandwidth
F. SEMANTIC (Implicit Semantic Dependency)

Functionally related but no explicit code association
Example: sender's packet_size should â‰¤ receiver's buffer_size
2. Analysis Steps
Check if caller passes values to callee through instantiation parameters
Identify semantic relationships (width, depth, enable, configuration, etc.)
Infer implicit constraints (e.g., width matching, capacity limits)
Judge coupling strength and confidence level
3. Confidence Assessment
high: Explicit association in code (e.g., parameter passing, calculation formula)
medium: Strong semantic correlation (e.g., data path width matching)
low: Speculative relationship (e.g., possible resource constraints)
## Analysis Task
Please synthesize all the above code contexts and analyze the coupling relationship between these two parameter clusters.

Focus on:

Existence of coupling: Are there dependencies or constraints between parameters from the two clusters?
Coupling types: Identify the pattern(s) listed above
Specific parameter pairs: List all discovered parameter coupling pairs
## Output Format
Output ONLY JSON, no other text

JSON
{{
  "cluster_pair": ["{cluster1}", "{cluster2}"],
  "has_coupling": true,
  "analysis_summary": "One sentence summarizing the relationship between these two clusters",
  "couplings": [
    {{
      "param1": "Parameter name from {cluster1}",
      "param2": "Parameter name from {cluster2}",
      "param1_cluster": "{cluster1}",
      "param2_cluster": "{cluster2}",
      "type": "DIRECT_PASS | DERIVATION | CONSTRAINT | CONDITIONAL | RESOURCE | SEMANTIC",
      "description": "Clear description of this coupling relationship",
      "rule": "Formalized rule (e.g., A=B, A>=B, A=log2(B))",
      "confidence": "high | medium | low",
      "reasoning": "Brief explanation of why this coupling exists",
      "evidence_contexts": [1, 2]
    }}
  ]
}}
Notes:

If no coupling found, return {{"has_coupling": false, "cluster_pair": ["{cluster1}", "{cluster2}"], "couplings": []}}

evidence_contexts indicates which context numbers support this coupling

Focus on actually existing, meaningful coupling relationships, avoid speculation

Prioritize high-confidence couplings """ 
        return prompt
    def analyze_all(self, max_pairs: int = None): 
        """Analyze all cluster pairs"""
        print("="*70)
        print("ğŸ¤– Step 4: LLM Analysis - Cluster Pairs")
        print("="*70)
        
        pairs_to_analyze = self.cluster_pairs[:max_pairs] if max_pairs else self.cluster_pairs
        
        print(f"Preparing to analyze {len(pairs_to_analyze)} cluster pair(s)\n")
        
        results = []
        
        for i, pair_task in enumerate(pairs_to_analyze, 1):
            cluster1, cluster2 = pair_task['cluster_pair']
            context_count = pair_task['context_count']
            
            print(f"[{i}/{len(pairs_to_analyze)}] Analyzing: ({cluster1}, {cluster2})")
            print(f"           Contexts: {context_count}", end=' ')
            
            if context_count == 0:
                print("âš ï¸  No contexts (skipped)")
                continue
            
            prompt = self.generate_prompt(pair_task)
            
            try:
                analysis = self.call_llm(prompt)
                
                if analysis and analysis.get('has_coupling'):
                    coupling_count = len(analysis.get('couplings', []))
                    print(f"âœ… Found {coupling_count} coupling(s)")
                    
                    results.append({
                        'cluster_pair': pair_task['cluster_pair'],
                        'contexts': pair_task['contexts'],
                        'context_count': context_count,
                        'has_intra_file': pair_task['has_intra_file'],
                        'has_inter_file': pair_task['has_inter_file'],
                        'analysis': analysis
                    })
                else:
                    print(f"â– No coupling")
            
            except Exception as e:
                print(f"âŒ Error: {e}")
        
        print(f"\nâœ… LLM analysis completed: {len(results)} cluster pair(s) with coupling found\n")
        
        return results
class CouplingExtractor:
    """ä»LLMç»“æœæå–è€¦åˆå…³ç³»"""
    def __init__(self, llm_results: List[Dict]):
        self.llm_results = llm_results

    def extract(self) -> List[Dict]:
        """Extract all parameter-level couplings with cluster information"""
        
        print("="*70)
        print("ğŸ“‹ Step 6: Extracting Parameter-Level Couplings")
        print("="*70)
        
        all_couplings = []
        
        for result in self.llm_results:
            cluster1, cluster2 = result['cluster_pair']
            analysis = result['analysis']
            contexts = result['contexts']
            
            for coupling in analysis.get('couplings', []):
                # Enrich coupling with cluster and context information
                enriched_coupling = {
                    'param1': coupling.get('param1'),
                    'param2': coupling.get('param2'),
                    'param1_cluster': coupling.get('param1_cluster', cluster1),
                    'param2_cluster': coupling.get('param2_cluster', cluster2),
                    'type': coupling.get('type'),
                    'description': coupling.get('description'),
                    'rule': coupling.get('rule'),
                    'confidence': coupling.get('confidence', 'medium'),
                    'reasoning': coupling.get('reasoning', ''),
                    'evidence_contexts': coupling.get('evidence_contexts', []),
                    'context_count': result['context_count'],
                    'has_intra_file': result['has_intra_file'],
                    'has_inter_file': result['has_inter_file']
                }
                
                all_couplings.append(enriched_coupling)
        
        print(f"âœ… Extracted {len(all_couplings)} parameter coupling(s)\n")
        
        return all_couplings

    def build_graph(self, couplings: List[Dict]) -> nx.DiGraph:
        """Build parameter coupling graph"""
        
        G = nx.DiGraph()
        
        for c in couplings:
            p1 = c['param1']
            p2 = c['param2']
            
            if p1 and p2:
                # Add nodes with cluster information
                G.add_node(p1, cluster=c['param1_cluster'])
                G.add_node(p2, cluster=c['param2_cluster'])
                
                # Add edge with coupling information
                G.add_edge(
                    p1, p2,
                    type=c['type'],
                    description=c['description'],
                    rule=c['rule'],
                    confidence=c['confidence']
                )
        
        return G

    def generate_summary(self, couplings: List[Dict]) -> Dict:
        """Generate statistical summary"""
        
        type_counts = defaultdict(int)
        conf_counts = defaultdict(int)
        cluster_pair_counts = defaultdict(int)
        
        for c in couplings:
            type_counts[c['type']] += 1
            conf_counts[c['confidence']] += 1
            
            # Count cluster pair combinations
            c1 = c['param1_cluster']
            c2 = c['param2_cluster']
            pair_key = tuple(sorted([c1, c2]))
            cluster_pair_counts[pair_key] += 1
        
        unique_params = set()
        for c in couplings:
            if c['param1']:
                unique_params.add(c['param1'])
            if c['param2']:
                unique_params.add(c['param2'])
        
        return {
            'total_couplings': len(couplings),
            'unique_parameters': len(unique_params),
            'unique_cluster_pairs': len(cluster_pair_counts),
            'by_type': dict(type_counts),
            'by_confidence': dict(conf_counts),
            'by_cluster_pair': dict(cluster_pair_counts),
            'high_confidence_count': conf_counts.get('high', 0),
            'medium_confidence_count': conf_counts.get('medium', 0),
            'low_confidence_count': conf_counts.get('low', 0)
        }


def build_coupling_matrix(llm_results: List[Dict], clusters_def: Dict, used_clusters: Set[str] = None) -> Dict:
    """Build cluster-to-cluster coupling matrix from LLM results
    
    Args:
        llm_results: Results from LLMCouplingAnalyzer.analyze_all()
        clusters_def: Original cluster definitions
        used_clusters: Set of actually used clusters (optional)
    
    Returns:
        Nested dict: cluster1 -> cluster2 -> coupling info
    """
    
    print("="*70)
    print("ğŸ“Š Step 5: Building Cluster Coupling Matrix")
    print("="*70)
    
    # å¦‚æœæ²¡æœ‰æä¾›used_clustersï¼Œåˆ™ä»llm_resultsä¸­æ¨æ–­
    if used_clusters is None:
        used_clusters = set()
        for result in llm_results:
            c1, c2 = result['cluster_pair']
            used_clusters.add(c1)
            used_clusters.add(c2)
    
    # åªä¸ºä½¿ç”¨è¿‡çš„ç°‡æ„å»ºçŸ©é˜µ
    cluster_list = sorted(list(used_clusters))
    
    # Initialize matrix (only for used clusters)
    matrix = {
        c1: {
            c2: {
                'has_coupling': False,
                'coupling_count': 0,
                'context_count': 0
            } 
            for c2 in cluster_list
        }
        for c1 in cluster_list
    }
    
    # Fill in results
    for result in llm_results:
        c1, c2 = result['cluster_pair']
        analysis = result['analysis']
        couplings = analysis.get('couplings', [])
        
        coupling_info = {
            'has_coupling': True,
            'coupling_count': len(couplings),
            'context_count': result['context_count'],
            'has_intra_file': result['has_intra_file'],
            'has_inter_file': result['has_inter_file'],
            'summary': analysis.get('analysis_summary', ''),
            'couplings': couplings
        }
        
        # Symmetric fill (since cluster pairs are unordered)
        matrix[c1][c2] = coupling_info
        matrix[c2][c1] = coupling_info
    
    # Generate statistics
    total_pairs_analyzed = len(llm_results)
    total_couplings = sum(r['analysis'].get('couplings', []) for r in llm_results)
    total_couplings_count = sum(len(couplings) for couplings in total_couplings)
    
    print(f"âœ… Matrix built:")
    print(f"   Used clusters: {len(cluster_list)}")
    print(f"   Total defined clusters: {len(clusters_def)}")
    print(f"   Analyzed cluster pairs: {total_pairs_analyzed}")
    print(f"   Coupled cluster pairs: {total_pairs_analyzed}")
    print(f"   Total parameter couplings: {total_couplings_count}\n")
    
    return matrix
def main(): 
    """ä¸»æµç¨‹ - ç°‡å¯¹ä¸­å¿ƒåˆ†æ"""
    print("\n" + "="*70)
    print("ğŸš€ å‚æ•°è€¦åˆå…³ç³»åˆ†æç³»ç»Ÿ (Cluster Pair Centric)")
    print("="*70 + "\n")

    # é…ç½®è·¯å¾„
    dependency_json = Path("dependency_analysis.json")
    clusters_json = Path("clusters.json")
    params_file = Path("cfg_params/fdae_top_template.in_pdt")
    driver_root = Path("driver/")

    # Step 1: åŠ è½½æ•°æ®ï¼ˆåŒ…æ‹¬å®Œæ•´å‚æ•°é…ç½®ï¼‰
    loader = DependencyLoader(dependency_json, clusters_json, params_file)
    loader.load()

    # Step 2: å­—ç¬¦ä¸²åŒ¹é…ï¼ˆé™„åŠ å®Œæ•´å‚æ•°ä¿¡æ¯ï¼‰
    matcher = StringMatcher(driver_root, loader.clusters, loader.params_info)
    candidates = matcher.scan_all()

    # ä¿å­˜candidates
    with open('candidates.json', 'w', encoding='utf-8') as f:
        json.dump(candidates, f, indent=2, ensure_ascii=False)
    print("ğŸ’¾ å·²ä¿å­˜: candidates.json\n")

    # Step 3: æ„å»ºç°‡å¯¹ä»»åŠ¡ï¼ˆæ”¹é€ ç‰ˆ - ä»¥ç°‡å¯¹ä¸ºä¸­å¿ƒï¼Œè‡ªåŠ¨è¿‡æ»¤æœªä½¿ç”¨çš„ç°‡ï¼‰
    cluster_pair_builder = ClusterPairBuilder(
        loader.dependency_data, 
        candidates,
        loader.clusters  # ä¼ å…¥ç°‡å®šä¹‰
    )
    cluster_pairs = cluster_pair_builder.build_pairs()

    # ä¿å­˜ç°‡å¯¹ä»»åŠ¡ï¼ˆç®€åŒ–ç‰ˆï¼Œç”¨äºæŸ¥çœ‹ï¼‰
    with open('cluster_pair_tasks.json', 'w', encoding='utf-8') as f:
        # åªä¿å­˜å…³é”®ä¿¡æ¯ï¼Œé¿å…åµŒå¥—å¯¹è±¡åºåˆ—åŒ–é—®é¢˜
        simplified_tasks = []
        for task in cluster_pairs:
            simplified_tasks.append({
                'cluster_pair': task['cluster_pair'],
                'context_count': task['context_count'],
                'has_intra_file': task['has_intra_file'],
                'has_inter_file': task['has_inter_file']
            })
        json.dump(simplified_tasks, f, indent=2, ensure_ascii=False)
    print("ğŸ’¾ å·²ä¿å­˜: cluster_pair_tasks.json\n")

    # Step 4: LLMåˆ†æç°‡å¯¹ï¼ˆæ”¹é€ ç‰ˆï¼‰
    llm_analyzer = LLMCouplingAnalyzer(cluster_pairs, loader.clusters)
    
    # æµ‹è¯•æ¨¡å¼ï¼šåªåˆ†æå‰5å¯¹
    # llm_results = llm_analyzer.analyze_all(max_pairs=5)
    
    # å®Œæ•´åˆ†ææ¨¡å¼ï¼š
    llm_results = llm_analyzer.analyze_all()

    # ä¿å­˜LLMåˆ†æç»“æœ
    with open('cluster_pair_couplings.json', 'w', encoding='utf-8') as f:
        # åºåˆ—åŒ–æ—¶å¤„ç†å¯èƒ½çš„å¤æ‚å¯¹è±¡
        json.dump(llm_results, f, indent=2, ensure_ascii=False, default=str)
    print("ğŸ’¾ å·²ä¿å­˜: cluster_pair_couplings.json\n")

    # Step 5: æ„å»ºç°‡å¯¹è€¦åˆçŸ©é˜µï¼ˆåªåŒ…å«ä½¿ç”¨çš„ç°‡ï¼‰
    coupling_matrix = build_coupling_matrix(llm_results, loader.clusters)
    
    with open('cluster_coupling_matrix.json', 'w', encoding='utf-8') as f:
        json.dump(coupling_matrix, f, indent=2, ensure_ascii=False)
    print("ğŸ’¾ å·²ä¿å­˜: cluster_coupling_matrix.json\n")

    # Step 6: æå–å‚æ•°çº§åˆ«çš„è€¦åˆï¼ˆå¯é€‰ï¼‰
    extractor = CouplingExtractor(llm_results)
    param_couplings = extractor.extract()

    # ä¿å­˜å‚æ•°çº§åˆ«çš„è€¦åˆ
    with open('extracted_param_couplings.json', 'w', encoding='utf-8') as f:
        json.dump(param_couplings, f, indent=2, ensure_ascii=False)
    print("ğŸ’¾ å·²ä¿å­˜: extracted_param_couplings.json\n")

    # ç”Ÿæˆç»Ÿè®¡æ‘˜è¦
    summary = extractor.generate_summary(param_couplings)
    with open('param_couplings_summary.json', 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    print("ğŸ’¾ å·²ä¿å­˜: param_couplings_summary.json\n")

    # æ„å»ºå‚æ•°è€¦åˆå›¾
    graph = extractor.build_graph(param_couplings)
    nx.write_gexf(graph, 'coupling_graph.gexf')
    print("ğŸ’¾ å·²ä¿å­˜: coupling_graph.gexf\n")

    # æ‰“å°æœ€ç»ˆç»Ÿè®¡
    print("="*70)
    print("ğŸ“Š æœ€ç»ˆç»Ÿè®¡")
    print("="*70)
    
    # ç°‡å¯¹çº§åˆ«ç»Ÿè®¡
    total_cluster_pairs = len(cluster_pairs)
    coupled_cluster_pairs = len(llm_results)
    
    print(f"\nã€ç°‡å¯¹çº§åˆ«ç»Ÿè®¡ã€‘")
    print(f"  åˆ†æçš„ç°‡å¯¹æ•°: {total_cluster_pairs}")
    if total_cluster_pairs > 0:
        print(f"  æœ‰è€¦åˆçš„ç°‡å¯¹: {coupled_cluster_pairs} ({coupled_cluster_pairs/total_cluster_pairs*100:.1f}%)")
    else:
        print(f"  æœ‰è€¦åˆçš„ç°‡å¯¹: 0")
    
    # å‚æ•°çº§åˆ«ç»Ÿè®¡
    print(f"\nã€å‚æ•°çº§åˆ«ç»Ÿè®¡ã€‘")
    print(f"  æ€»å‚æ•°è€¦åˆæ•°: {summary['total_couplings']}")
    print(f"  æ¶‰åŠå‚æ•°æ•°é‡: {summary['unique_parameters']}")
    print(f"  æ¶‰åŠç°‡å¯¹æ•°é‡: {summary['unique_cluster_pairs']}")
    
    print(f"\nã€æŒ‰è€¦åˆç±»å‹ã€‘")
    for coupling_type, count in summary['by_type'].items():
        print(f"  - {coupling_type}: {count}")
    
    print(f"\nã€æŒ‰ç½®ä¿¡åº¦ã€‘")
    print(f"  - High: {summary['high_confidence_count']}")
    print(f"  - Medium: {summary['medium_confidence_count']}")
    print(f"  - Low: {summary['low_confidence_count']}")
    
    # Top 5 æœ€å¤šè€¦åˆçš„ç°‡å¯¹
    if summary['by_cluster_pair']:
        print(f"\nã€è€¦åˆæœ€å¤šçš„ç°‡å¯¹ Top 5ã€‘")
        top_pairs = sorted(
            summary['by_cluster_pair'].items(), 
            key=lambda x: x[1], 
            reverse=True
        )[:5]
        for i, (pair, count) in enumerate(top_pairs, 1):
            print(f"  {i}. {pair[0]} â†” {pair[1]}: {count} ä¸ªè€¦åˆ")
    
    print("\n" + "="*70)
    print("âœ… åˆ†æå®Œæˆï¼")
    print("="*70)
    
    # è¾“å‡ºæ–‡ä»¶æ¸…å•
    print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print("  1. candidates.json              - å‚æ•°ç°‡åœ¨æ–‡ä»¶ä¸­çš„åŒ¹é…ç»“æœ")
    print("  2. cluster_pair_tasks.json      - éœ€è¦åˆ†æçš„ç°‡å¯¹ä»»åŠ¡åˆ—è¡¨")
    print("  3. cluster_pair_couplings.json  - LLMåˆ†æçš„ç°‡å¯¹è€¦åˆç»“æœ")
    print("  4. cluster_coupling_matrix.json - ç°‡å¯¹è€¦åˆçŸ©é˜µ (ä»…åŒ…å«ä½¿ç”¨çš„ç°‡)")
    print("  5. extracted_param_couplings.json - å‚æ•°çº§åˆ«çš„è€¦åˆåˆ—è¡¨")
    print("  6. param_couplings_summary.json - ç»Ÿè®¡æ‘˜è¦")
    print("  7. coupling_graph.gexf          - å‚æ•°è€¦åˆå…³ç³»å›¾\n")


if __name__ == '__main__': 
    main()