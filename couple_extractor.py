"""
å®Œæ•´çš„å‚æ•°è€¦åˆåˆ†æç³»ç»Ÿ
åŸºäºå·²æ„å»ºçš„ä»£ç ä¾èµ–å…³ç³» (dependency_analysis.json)
ä½œè€…: @Fuhj-better
æ—¥æœŸ: 2025-11-11
"""

import json
import os
import re
from pathlib import Path
from typing import List, Dict, Any, Set, Tuple
from collections import defaultdict
from dataclasses import dataclass, asdict
import networkx as nx


# =============================================================================
# æ•°æ®ç±»å®šä¹‰
# =============================================================================

@dataclass
class ParamInfo:
    """å‚æ•°é…ç½®ä¿¡æ¯"""
    name: str
    type: str
    default: str
    range: str
    comment: str

@dataclass
class FileMatchResult:
    """æ–‡ä»¶åŒ¹é…ç»“æœ"""
    file_path: str
    matched_params: List[str]
    params_info: List[Dict]
    match_count: int
    contexts: List[Dict]


# =============================================================================
# Step 1: åŠ è½½ä¾èµ–ä¿¡æ¯å’Œå‚æ•°é…ç½®
# =============================================================================

class DependencyLoader:
    """åŠ è½½ä»£ç ä¾èµ–ä¿¡æ¯å’Œå‚æ•°é…ç½®"""
    
    def __init__(self, dependency_json: Path, clusters_json: Path, params_file: Path):
        self.dependency_json = dependency_json
        self.clusters_json = clusters_json
        self.params_file = params_file
        
        self.dependency_data = None
        self.clusters = None
        self.params_info = {}  # å­˜å‚¨å®Œæ•´çš„å‚æ•°ä¿¡æ¯

    def parse_param_line(self, line: str) -> ParamInfo:
        """
        è§£æå‚æ•°é…ç½®è¡Œ
        æ ¼å¼: PARAM_NAME(type) value (range) [# comment]
        ä¾‹å¦‚: FDAE_WIDTH(int) 32 (1-1024) # Data width
        """
        line = line.strip()
        if not line or line.startswith('#'):
            return None
        
        # æå–æ³¨é‡Š
        comment = ''
        if '#' in line:
            line, comment = line.split('#', 1)
            comment = comment.strip()
            line = line.strip()
        
        # åŒ¹é…æ ¼å¼: NAME(type) value (range)
        pattern = r'(\S+)\s*\((\w+)\)\s+(\S+)(?:\s+\(([^)]+)\))?'
        match = re.match(pattern, line)
        
        if not match:
            return None
        
        return ParamInfo(
            name=match.group(1),
            type=match.group(2),
            default=match.group(3),
            range=match.group(4) if match.group(4) else '',
            comment=comment
        )
    
    def load(self):
        """åŠ è½½æ‰€æœ‰å¿…è¦æ•°æ®"""
        
        print("="*70)
        print("ğŸ“‚ Step 1: åŠ è½½æ•°æ®")
        print("="*70)
        
        # 1. åŠ è½½ä¾èµ–åˆ†æç»“æœ
        if self.dependency_json.exists():
            with open(self.dependency_json, 'r', encoding='utf-8') as f:
                self.dependency_data = json.load(f)
            
            summary = self.dependency_data.get('dependency_analysis', {}).get('summary', {})
            print(f"âœ… åŠ è½½ä»£ç ä¾èµ–ä¿¡æ¯:")
            print(f"   - æ–‡ä»¶æ•°: {summary.get('total_files', 0)}")
            print(f"   - ä¾èµ–å…³ç³»: {summary.get('total_dependencies', 0)}")
            print(f"   - æ¨¡å—å®ä¾‹åŒ–: {summary.get('module_dependencies', 0)}")
        else:
            print(f"âš ï¸  æœªæ‰¾åˆ° {self.dependency_json}")
            self.dependency_data = {}
        
        # 2. åŠ è½½å‚æ•°ç°‡
        if self.clusters_json.exists():
            with open(self.clusters_json, 'r', encoding='utf-8') as f:
                self.clusters = json.load(f)
            print(f"âœ… åŠ è½½å‚æ•°ç°‡: {len(self.clusters)} ä¸ªç°‡")
        else:
            print(f"âš ï¸  æœªæ‰¾åˆ° {self.clusters_json}")
            self.clusters = {}
        
        # 3. åŠ è½½å¹¶è§£æå‚æ•°é…ç½®æ–‡ä»¶
        if self.params_file.exists():
            with open(self.params_file, 'r', encoding='utf-8') as f:
                for line in f:
                    param_info = self.parse_param_line(line)
                    if param_info:
                        self.params_info[param_info.name] = param_info
            
            print(f"âœ… åŠ è½½å‚æ•°é…ç½®: {len(self.params_info)} ä¸ªå‚æ•°")
            
            # æ‰“å°ç¤ºä¾‹
            if self.params_info:
                first_param = next(iter(self.params_info.values()))
                print(f"   ç¤ºä¾‹å‚æ•°: {first_param.name}")
                print(f"     - ç±»å‹: {first_param.type}")
                print(f"     - é»˜è®¤å€¼: {first_param.default}")
                print(f"     - èŒƒå›´: {first_param.range}")
                if first_param.comment:
                    print(f"     - è¯´æ˜: {first_param.comment}")
        else:
            print(f"âš ï¸  æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ {self.params_file}")
        
        print()
        return self


# =============================================================================
# Step 2: å­—ç¬¦ä¸²åŒ¹é… - æ‰¾å‡ºå“ªäº›æ–‡ä»¶ä½¿ç”¨äº†å“ªäº›å‚æ•°
# =============================================================================

class StringMatcher:
    """å­—ç¬¦ä¸²åŒ¹é…ï¼šæŒ‰ç°‡æ‰«ææ–‡ä»¶ä¸­çš„å‚æ•°"""
    
    def __init__(self, driver_root: Path, clusters: Dict, params_info: Dict[str, ParamInfo]):
        self.driver_root = driver_root
        self.clusters = clusters
        self.params_info = params_info

    def remove_noise(self, content: str) -> str:
        """ç§»é™¤æ³¨é‡Šå’Œå­—ç¬¦ä¸²å­—é¢é‡ï¼ˆå‡å°‘è¯¯åŒ¹é…ï¼‰"""
        # ç§»é™¤å•è¡Œæ³¨é‡Š //
        content = re.sub(r'//.*?$', '', content, flags=re.MULTILINE)
        # ç§»é™¤å¤šè¡Œæ³¨é‡Š /* */
        content = re.sub(r'/\*.*?\*/', '', content, flags=re.DOTALL)
        # ç§»é™¤å­—ç¬¦ä¸² "..."
        content = re.sub(r'"[^"]*"', '""', content)
        return content
    
    def extract_context(self, content: str, match_pos: int, window: int = 200) -> Dict:
        """æå–åŒ¹é…ä½ç½®çš„ä»£ç ä¸Šä¸‹æ–‡"""
        lines = content[:match_pos].count('\n')
        start = max(0, match_pos - window)
        end = min(len(content), match_pos + window)
        
        return {
            'line_number': lines + 1,
            'snippet': content[start:end],
            'position': match_pos
        }
    
    def match_params_in_file(self, file_path: Path, cluster_params: List[str]) -> FileMatchResult:
        """åœ¨å•ä¸ªæ–‡ä»¶ä¸­åŒ¹é…æŒ‡å®šç°‡çš„å‚æ•°"""
        
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                raw_content = f.read()
        except Exception as e:
            return None
        
        # æ¸…ç†å†…å®¹
        cleaned_content = self.remove_noise(raw_content)
        
        matched_params = []
        contexts = []
        
        # åªæ‰«æè¯¥ç°‡çš„å‚æ•°
        for param_name in cluster_params:
            if param_name not in self.params_info:
                continue
            
            # ä½¿ç”¨è¯è¾¹ç•Œæ­£åˆ™ï¼ˆé¿å…éƒ¨åˆ†åŒ¹é…ï¼‰
            pattern = r'\b' + re.escape(param_name) + r'\b'
            matches = list(re.finditer(pattern, cleaned_content))
            
            if matches:
                matched_params.append(param_name)
                
                # ä¿å­˜å‰3ä¸ªåŒ¹é…ä½ç½®çš„ä¸Šä¸‹æ–‡
                for match in matches[:3]:
                    ctx = self.extract_context(cleaned_content, match.start())
                    ctx['param'] = param_name
                    contexts.append(ctx)
        
        if not matched_params:
            return None
        
        # æ„å»ºå‚æ•°è¯¦ç»†ä¿¡æ¯
        params_with_info = []
        for param in matched_params:
            param_config = self.params_info[param]
            params_with_info.append({
                'name': param_config.name,
                'type': param_config.type,
                'default': param_config.default,
                'range': param_config.range,
                'comment': param_config.comment
            })
        
        return FileMatchResult(
            file_path=str(file_path),
            matched_params=sorted(matched_params),
            params_info=params_with_info,
            match_count=len(matched_params),
            contexts=contexts
        )
    
    def scan_cluster(self, cluster_name: str, cluster_params: List[str]) -> List[Dict]:
        """æ‰«æä¸€ä¸ªå‚æ•°ç°‡"""
        
        results = []
        
        # è·å–æ‰€æœ‰æ–‡ä»¶
        all_files = [f for f in self.driver_root.rglob('*') if f.is_file()]
        print(f"å…± {len(all_files)} ä¸ªæ–‡ä»¶")
        
        for file_path in all_files:
            result = self.match_params_in_file(file_path, cluster_params)
            if result:
                results.append({
                    'file': result.file_path,
                    'matched_params': result.matched_params,
                    'params_info': result.params_info,
                    'match_count': result.match_count,
                    'contexts': result.contexts
                })
        
        # æŒ‰åŒ¹é…æ•°é‡æ’åº
        results.sort(key=lambda x: x['match_count'], reverse=True)
        
        return results
    
    def scan_all(self) -> Dict[str, List[Dict]]:
        """æŒ‰ç°‡æ‰«ææ‰€æœ‰æ–‡ä»¶"""
        
        print("="*70)
        print("ğŸ” Step 2: å­—ç¬¦ä¸²åŒ¹é…æ‰«æï¼ˆæŒ‰ç°‡ï¼‰")
        print("="*70)
        
        all_files = [f for f in self.driver_root.rglob('*') if f.is_file()]
        print(f"å…± {len(all_files)} ä¸ªæ–‡ä»¶")
        print(f"å…± {len(self.clusters)} ä¸ªå‚æ•°ç°‡\n")
        
        candidates = {}
        
        # ä¸ºæ¯ä¸ªç°‡ç‹¬ç«‹æ‰«æ
        for cluster_name, cluster_params in self.clusters.items():
            print(f"ğŸ“¦ æ‰«æç°‡: {cluster_name} ({len(cluster_params)} ä¸ªå‚æ•°)...", end=' ')
            
            cluster_results = self.scan_cluster(cluster_name, cluster_params)
            candidates[cluster_name] = cluster_results
            
            print(f"æ‰¾åˆ° {len(cluster_results)} ä¸ªæ–‡ä»¶")
        
        # ç»Ÿè®¡
        total_matches = sum(len(files) for files in candidates.values())
        print(f"\nâœ… æ€»è®¡: {total_matches} ä¸ªæ–‡ä»¶-ç°‡åŒ¹é…")
        print(f"âœ… å·²é™„åŠ å®Œæ•´å‚æ•°é…ç½®ä¿¡æ¯\n")
        
        return candidates


# =============================================================================
# Step 3: åŸºäºä¾èµ–å…³ç³»æ„å»ºæ–‡ä»¶å¯¹
# =============================================================================

class FilePairBuilder:
    """åŸºäºä»£ç ä¾èµ–å…³ç³»æ„å»ºæ–‡ä»¶å¯¹"""
    
    def __init__(self, dependency_data: Dict, candidates: Dict):
        self.dependency_data = dependency_data
        self.candidates = candidates
        
    def build_pairs(self) -> List[Dict]:
        """ä» dependency_analysis.json æå–æ–‡ä»¶å¯¹ï¼Œæ”¯æŒå•ç«¯å¤šç°‡å’ŒåŒç«¯åˆ†æ"""
        
        print("="*70)
        print("ğŸ”— Step 3: æ„å»ºæ–‡ä»¶è°ƒç”¨å¯¹ï¼ˆåŸºäºä¾èµ–åˆ†æï¼‰")
        print("="*70)
        
        module_deps = (self.dependency_data
                      .get('dependency_analysis', {})
                      .get('dependency_relationships', {})
                      .get('module_dependencies', []))
        
        file_pairs = []
        
        # æ„å»ºæ–‡ä»¶->å‚æ•°æ˜ å°„ï¼ˆåŒ…å«å®Œæ•´ä¿¡æ¯ï¼‰
        file_to_params = {}
        for cluster_name, files in self.candidates.items():
            for f in files:
                fp = f['file']
                if fp not in file_to_params:
                    file_to_params[fp] = {
                        'params': set(),
                        'params_info': [],
                        'clusters': set(),
                        'contexts': []
                    }
                file_to_params[fp]['params'].update(f['matched_params'])
                file_to_params[fp]['params_info'].extend(f['params_info'])
                file_to_params[fp]['clusters'].add(cluster_name)
                file_to_params[fp]['contexts'].extend(f['contexts'])
        
        # æ„å»ºæ–‡ä»¶å¯¹
        for dep in module_deps:
            caller = dep.get('source_path')
            callee = dep.get('target_path')
            
            caller_info = file_to_params.get(caller)
            callee_info = file_to_params.get(callee)
            
            # æƒ…å†µ1: åŒç«¯éƒ½æœ‰å‚æ•° - åˆ†æè·¨æ–‡ä»¶è€¦åˆ
            if caller_info and callee_info:
                file_pairs.append({
                    'type': 'INTER_FILE',  # è·¨æ–‡ä»¶åˆ†æ
                    'caller_file': caller,
                    'callee_file': callee,
                    'module': dep.get('module_type'),
                    'instance': dep.get('instance_name'),
                    'instantiation_code': dep.get('description', ''),
                    'caller_params': sorted(list(caller_info['params'])),
                    'caller_params_info': caller_info['params_info'],
                    'callee_params': sorted(list(callee_info['params'])),
                    'callee_params_info': callee_info['params_info'],
                    'caller_clusters': sorted(list(caller_info['clusters'])),
                    'callee_clusters': sorted(list(callee_info['clusters']))
                })
            
            # æƒ…å†µ2: åªæœ‰calleræœ‰å‚æ•°ï¼Œä¸”å±äºå¤šä¸ªç°‡ - åˆ†æç°‡é—´è€¦åˆ
            elif caller_info and not callee_info and len(caller_info['clusters']) > 1:
                file_pairs.append({
                    'type': 'INTRA_FILE_MULTI_CLUSTER',  # å•æ–‡ä»¶å¤šç°‡åˆ†æ
                    'file': caller,
                    'role': 'caller',
                    'module': dep.get('module_type'),
                    'instance': dep.get('instance_name'),
                    'instantiation_code': dep.get('description', ''),
                    'params': sorted(list(caller_info['params'])),
                    'params_info': caller_info['params_info'],
                    'clusters': sorted(list(caller_info['clusters'])),
                    'other_file': callee  # è®°å½•å…³è”æ–‡ä»¶
                })
            
            # æƒ…å†µ3: åªæœ‰calleeæœ‰å‚æ•°ï¼Œä¸”å±äºå¤šä¸ªç°‡ - åˆ†æç°‡é—´è€¦åˆ
            elif callee_info and not caller_info and len(callee_info['clusters']) > 1:
                file_pairs.append({
                    'type': 'INTRA_FILE_MULTI_CLUSTER',  # å•æ–‡ä»¶å¤šç°‡åˆ†æ
                    'file': callee,
                    'role': 'callee',
                    'module': dep.get('module_type'),
                    'instance': dep.get('instance_name'),
                    'instantiation_code': dep.get('description', ''),
                    'params': sorted(list(callee_info['params'])),
                    'params_info': callee_info['params_info'],
                    'clusters': sorted(list(callee_info['clusters'])),
                    'other_file': caller  # è®°å½•å…³è”æ–‡ä»¶
                })
        
        # ç»Ÿè®¡
        inter_file_count = sum(1 for p in file_pairs if p['type'] == 'INTER_FILE')
        intra_file_count = sum(1 for p in file_pairs if p['type'] == 'INTRA_FILE_MULTI_CLUSTER')
        
        print(f"âœ… æ„å»ºäº† {len(file_pairs)} ä¸ªåˆ†æä»»åŠ¡:")
        print(f"   - è·¨æ–‡ä»¶åˆ†æ (ä¸¤ç«¯éƒ½æœ‰å‚æ•°): {inter_file_count}")
        print(f"   - å•æ–‡ä»¶å¤šç°‡åˆ†æ (ä¸€ç«¯å¤šç°‡): {intra_file_count}")
        print(f"âœ… æ¯ä¸ªä»»åŠ¡éƒ½åŒ…å«å®Œæ•´çš„å‚æ•°é…ç½®ä¿¡æ¯\n")
        
        return file_pairs


# =============================================================================
# Step 4: LLM åˆ†ææ–‡ä»¶å¯¹
# =============================================================================

class LLMCouplingAnalyzer:
    """ä½¿ç”¨LLMåˆ†ææ–‡ä»¶å¯¹çš„å‚æ•°è€¦åˆ"""
    
    def __init__(self, file_pairs: List[Dict], clusters_def: Dict):
        self.file_pairs = file_pairs
        self.clusters_def = clusters_def  # ä¿å­˜åŸå§‹ç°‡å®šä¹‰ï¼Œç”¨äºåˆ†ç»„å‚æ•°
    
    def format_params_info(self, params_info: List[Dict]) -> str:
        """æ ¼å¼åŒ–å‚æ•°ä¿¡æ¯ä¸ºæ˜“è¯»æ–‡æœ¬"""
        lines = []
        for p in params_info:
            line = f"  - {p['name']} ({p['type']})"
            if p['default']:
                line += f" = {p['default']}"
            if p['range']:
                line += f" [{p['range']}]"
            if p['comment']:
                line += f"  // {p['comment']}"
            lines.append(line)
        return '\n'.join(lines)
    
    def format_params_by_cluster(self, params_info: List[Dict], clusters: List[str]) -> str:
        """æŒ‰ç°‡åˆ†ç»„å¹¶æ ¼å¼åŒ–å‚æ•°ä¿¡æ¯"""
        output_lines = []
        
        for cluster_name in clusters:
            cluster_params_list = self.clusters_def.get(cluster_name, [])
            cluster_params_set = set(cluster_params_list)
            
            # è¿‡æ»¤å±äºè¯¥ç°‡çš„å‚æ•°
            params_in_cluster = [p for p in params_info if p['name'] in cluster_params_set]
            
            if params_in_cluster:
                output_lines.append(f"\n### ç°‡: {cluster_name} ({len(params_in_cluster)} ä¸ªå‚æ•°)")
                for p in params_in_cluster:
                    line = f"  - {p['name']} ({p['type']})"
                    if p['default']:
                        line += f" = {p['default']}"
                    if p['range']:
                        line += f" [{p['range']}]"
                    if p['comment']:
                        line += f"  // {p['comment']}"
                    output_lines.append(line)
        
        return '\n'.join(output_lines)
    
    def generate_inter_file_prompt(self, pair: Dict) -> str:
        """ç”Ÿæˆè·¨æ–‡ä»¶åˆ†æçš„æç¤ºè¯ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        
        # æŒ‰ç°‡åˆ†ç»„æ˜¾ç¤ºå‚æ•°
        caller_params_text = self.format_params_by_cluster(
            pair['caller_params_info'], 
            pair['caller_clusters']
        )
        callee_params_text = self.format_params_by_cluster(
            pair['callee_params_info'], 
            pair['callee_clusters']
        )
        
        prompt = f"""# ç¡¬ä»¶å‚æ•°è€¦åˆåˆ†æä»»åŠ¡

## èƒŒæ™¯
è¿™æ˜¯ä¸€ä¸ªç¡¬ä»¶è®¾è®¡é¡¹ç›®çš„é…ç½®å‚æ•°åˆ†æã€‚é…ç½®å‚æ•°åœ¨ç¼–è¯‘æ—¶ç¡®å®šç¡¬ä»¶æ¨¡å—çš„è¡Œä¸ºç‰¹æ€§ï¼ˆå¦‚ä½å®½ã€æ·±åº¦ã€ä½¿èƒ½ç­‰ï¼‰ã€‚

## ä»»åŠ¡ç›®æ ‡
åˆ†æä¸¤ä¸ªæœ‰æ¨¡å—å®ä¾‹åŒ–å…³ç³»çš„æ–‡ä»¶ä¹‹é—´ï¼Œ**é…ç½®å‚æ•°çš„ä¾èµ–å’Œçº¦æŸå…³ç³»**ã€‚

---

## è°ƒç”¨è€…æ–‡ä»¶ï¼ˆå®ä¾‹åŒ–å…¶ä»–æ¨¡å—çš„æ–‡ä»¶ï¼‰
**æ–‡ä»¶**: `{Path(pair['caller_file']).name}`
**æ‰€å±å‚æ•°ç°‡**: {', '.join(pair['caller_clusters'])}

{caller_params_text}

---

## è¢«è°ƒç”¨æ–‡ä»¶ï¼ˆè¢«å®ä¾‹åŒ–çš„æ¨¡å—æ–‡ä»¶ï¼‰
**æ–‡ä»¶**: `{Path(pair['callee_file']).name}`
**æ‰€å±å‚æ•°ç°‡**: {', '.join(pair['callee_clusters'])}

{callee_params_text}

---

## å®ä¾‹åŒ–å…³ç³»
```
è°ƒç”¨è€…æ–‡ä»¶å®ä¾‹åŒ–äº†è¢«è°ƒç”¨æ–‡ä»¶ä¸­å®šä¹‰çš„æ¨¡å—
æ¨¡å—ç±»å‹: {pair['module']}
å®ä¾‹åç§°: {pair['instance']}
ä¸Šä¸‹æ–‡: {pair['instantiation_code']}
```

---

## åˆ†ææŒ‡å¯¼

### 1. ç†è§£ç¡¬ä»¶å‚æ•°è€¦åˆçš„å¸¸è§æ¨¡å¼

**A. ç›´æ¥å‚æ•°ä¼ é€’ (DIRECT_PASS)**
- è°ƒç”¨è€…é€šè¿‡å®ä¾‹åŒ–å‚æ•°ç›´æ¥ä¼ é€’ç»™è¢«è°ƒç”¨è€…
- ä¾‹å¦‚ï¼š`top_width` â†’ `fifo_width` (é€šè¿‡ `#(.WIDTH(top_width))`)

**B. æ´¾ç”Ÿè®¡ç®— (DERIVATION)**
- ä¸€ä¸ªå‚æ•°é€šè¿‡æ•°å­¦å…¬å¼è®¡ç®—å¾—åˆ°å¦ä¸€ä¸ªå‚æ•°
- ä¾‹å¦‚ï¼š`addr_width = log2(depth)`

**C. çº¦æŸå…³ç³» (CONSTRAINT)**
- å‚æ•°ä¹‹é—´å¿…é¡»æ»¡è¶³çš„ä¸ç­‰å¼æˆ–ç­‰å¼
- ä¾‹å¦‚ï¼š`input_width <= output_width` (é¿å…æ•°æ®æˆªæ–­)
- ä¾‹å¦‚ï¼š`cache_line_size % bus_width == 0` (å¯¹é½è¦æ±‚)

**D. æ¡ä»¶ä¾èµ– (CONDITIONAL)**
- æŸå‚æ•°çš„å€¼å†³å®šå¦ä¸€å‚æ•°çš„å–å€¼æˆ–æœ‰æ•ˆæ€§
- ä¾‹å¦‚ï¼š`if enable_ecc==1 then ecc_width=8 else ecc_width=0`

**E. èµ„æºçº¦æŸ (RESOURCE)**
- å¤šä¸ªå‚æ•°å…±äº«èµ„æºé™åˆ¶
- ä¾‹å¦‚ï¼š`num_channels * channel_width <= total_bandwidth`

**F. éšå¼è¯­ä¹‰ä¾èµ– (SEMANTIC)**
- åŠŸèƒ½ä¸Šç›¸å…³ä½†æ— æ˜¾å¼ä»£ç å…³è”
- ä¾‹å¦‚ï¼šå‘é€ç«¯çš„ `packet_size` åº” â‰¤ æ¥æ”¶ç«¯çš„ `buffer_size`

### 2. åˆ†ææ­¥éª¤
1. æ£€æŸ¥è°ƒç”¨è€…æ˜¯å¦é€šè¿‡å®ä¾‹åŒ–å‚æ•°ä¼ é€’å€¼ç»™è¢«è°ƒç”¨è€…
2. è¯†åˆ«å‚æ•°çš„è¯­ä¹‰å…³ç³»ï¼ˆä½å®½ã€æ·±åº¦ã€ä½¿èƒ½ã€é…ç½®ç­‰ï¼‰
3. æ¨æ–­éšå«çš„çº¦æŸæ¡ä»¶ï¼ˆå¦‚ä½å®½åŒ¹é…ã€å®¹é‡é™åˆ¶ç­‰ï¼‰
4. åˆ¤æ–­è€¦åˆçš„å¼ºåº¦å’Œç½®ä¿¡åº¦

### 3. ç½®ä¿¡åº¦è¯„ä¼°
- **high**: ä»£ç ä¸­æœ‰æ˜¾å¼å…³è”ï¼ˆå¦‚å‚æ•°ä¼ é€’ã€è®¡ç®—å…¬å¼ï¼‰
- **medium**: è¯­ä¹‰ä¸Šå¼ºç›¸å…³ï¼ˆå¦‚æ•°æ®é€šè·¯çš„ä½å®½åŒ¹é…ï¼‰
- **low**: æ¨æµ‹æ€§çš„å…³ç³»ï¼ˆå¦‚å¯èƒ½çš„èµ„æºçº¦æŸï¼‰

---

## è¾“å‡ºè¦æ±‚

**JSONæ ¼å¼**ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š

```json
{{
  "has_coupling": true,
  "analysis_summary": "ç®€è¦æ€»ç»“å‘ç°çš„ä¸»è¦è€¦åˆæ¨¡å¼ï¼ˆ1-2å¥è¯ï¼‰",
  "couplings": [
    {{
      "caller_param": "è°ƒç”¨è€…æ–‡ä»¶ä¸­çš„å‚æ•°å",
      "callee_param": "è¢«è°ƒç”¨è€…æ–‡ä»¶ä¸­çš„å‚æ•°å",
      "type": "DIRECT_PASS | DERIVATION | CONSTRAINT | CONDITIONAL | RESOURCE | SEMANTIC",
      "description": "ç”¨ä¸€å¥è¯æ¸…æ™°æè¿°è¿™ä¸ªè€¦åˆå…³ç³»",
      "rule": "å½¢å¼åŒ–è§„åˆ™ï¼ˆå¦‚ A=B, A>=B, if A then B, A=log2(B)ï¼‰",
      "confidence": "high | medium | low",
      "reasoning": "ä¸ºä»€ä¹ˆè®¤ä¸ºå­˜åœ¨è¿™ä¸ªè€¦åˆï¼ˆç®€çŸ­è¯´æ˜ï¼‰"
    }}
  ]
}}

**æ³¨æ„**ï¼š
- åªè¾“å‡ºJSONï¼Œä¸è¦å…¶ä»–è§£é‡Šæ–‡å­—
- å¦‚æœæ‰¾ä¸åˆ°ä»»ä½•è€¦åˆï¼Œè¿”å› `{{"has_coupling": false, "couplings": []}}`
- èšç„¦äº**å®é™…å­˜åœ¨çš„ã€æœ‰æ„ä¹‰çš„**è€¦åˆå…³ç³»ï¼Œé¿å…è‡†æµ‹
- ä¼˜å…ˆæ ‡æ³¨é«˜ç½®ä¿¡åº¦çš„è€¦åˆ

---

è¯·å¼€å§‹åˆ†æã€‚
""" 
        return prompt
    
    def generate_intra_file_prompt(self, pair: Dict) -> str:
        """ç”Ÿæˆå•æ–‡ä»¶å¤šç°‡åˆ†æçš„æç¤ºè¯ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        
        # æŒ‰ç°‡åˆ†ç»„æ˜¾ç¤ºå‚æ•°
        params_by_cluster = self.format_params_by_cluster(
            pair['params_info'], 
            pair['clusters']
        )
        
        prompt = f"""# å•æ–‡ä»¶å†…è·¨ç°‡å‚æ•°è€¦åˆåˆ†æ

## èƒŒæ™¯
è¿™æ˜¯ä¸€ä¸ªç¡¬ä»¶è®¾è®¡é¡¹ç›®çš„é…ç½®å‚æ•°åˆ†æã€‚ä¸€ä¸ªæ–‡ä»¶å¯èƒ½åŒ…å«å¤šä¸ªåŠŸèƒ½æ¨¡å—çš„å‚æ•°ï¼Œè¿™äº›å‚æ•°è¢«åˆ†åˆ°äº†ä¸åŒçš„**å‚æ•°ç°‡**ä¸­ã€‚

## ä»»åŠ¡ç›®æ ‡
åˆ†æåŒä¸€ä¸ªæ–‡ä»¶å†…ï¼Œ**ä¸åŒå‚æ•°ç°‡ä¹‹é—´çš„å‚æ•°è€¦åˆå…³ç³»**ã€‚

---

## ç›®æ ‡æ–‡ä»¶
**æ–‡ä»¶**: `{Path(pair['file']).name}`
**åœ¨è°ƒç”¨é“¾ä¸­çš„è§’è‰²**: {'è°ƒç”¨è€… (å®ä¾‹åŒ–å…¶ä»–æ¨¡å—)' if pair['role'] == 'caller' else 'è¢«è°ƒç”¨è€… (è¢«å…¶ä»–æ¨¡å—å®ä¾‹åŒ–)'}
**å…³è”æ–‡ä»¶**: `{Path(pair['other_file']).name}`

## å‚æ•°åˆ†å¸ƒ
è¯¥æ–‡ä»¶åŒ…å« **{len(pair['clusters'])} ä¸ªå‚æ•°ç°‡**ï¼Œå…± **{len(pair['params'])} ä¸ªå‚æ•°**ï¼š

{params_by_cluster}

## å®ä¾‹åŒ–ä¸Šä¸‹æ–‡
```
æ¨¡å—ç±»å‹: {pair['module']}
å®ä¾‹åç§°: {pair['instance']}
è¯´æ˜: {pair['instantiation_code']}
```

---

## åˆ†ææŒ‡å¯¼

### 1. ç†è§£è·¨ç°‡è€¦åˆçš„åœºæ™¯

åœ¨ç¡¬ä»¶è®¾è®¡ä¸­ï¼Œä¸åŒåŠŸèƒ½æ¨¡å—ï¼ˆç°‡ï¼‰çš„å‚æ•°å¯èƒ½å­˜åœ¨éšå¼çº¦æŸï¼š

**åœºæ™¯A: æ•°æ®é€šè·¯ä¸€è‡´æ€§**
- ä¾‹å¦‚ï¼šæ—¶é’Ÿç°‡çš„ `clk_freq` å½±å“ FIFOç°‡çš„ `depth`ï¼ˆæ»¡è¶³ååéœ€æ±‚ï¼‰

**åœºæ™¯B: èµ„æºå…±äº«çº¦æŸ**
- ä¾‹å¦‚ï¼šå¤šä¸ªDMAé€šé“çš„æ€»å¸¦å®½ä¸èƒ½è¶…è¿‡æ€»çº¿å¸¦å®½

**åœºæ™¯C: å±‚æ¬¡åŒ–æ´¾ç”Ÿ**
- ä¾‹å¦‚ï¼šé¡¶å±‚å‚æ•° `total_width` å†³å®šäº†å­æ¨¡å—çš„ `channel_width`

**åœºæ™¯D: ä½¿èƒ½å¼€å…³è”åŠ¨**
- ä¾‹å¦‚ï¼š`enable_feature_A==1` æ—¶è¦æ±‚ `feature_B_buffer_size >= 1024`

### 2. åˆ†ææ­¥éª¤
1. **è¯†åˆ«ç°‡çš„è¯­ä¹‰**ï¼šç†è§£æ¯ä¸ªç°‡ä»£è¡¨çš„åŠŸèƒ½æ¨¡å—
2. **æ£€æŸ¥å‚æ•°ç±»å‹**ï¼šä½å®½ã€æ·±åº¦ã€ä½¿èƒ½ã€é¢‘ç‡ç­‰
3. **æ¨æ–­ä¾èµ–é“¾**ï¼šæ˜¯å¦å­˜åœ¨"ç°‡Aå½±å“ç°‡B"çš„å…³ç³»
4. **è¯„ä¼°ç‹¬ç«‹æ€§**ï¼šå“ªäº›ç°‡ä¹‹é—´ç¡®å®æ— å…³è”

### 3. é‡ç‚¹å…³æ³¨
- ä¸åŒç°‡çš„å‚æ•°æ˜¯å¦åœ¨åŒä¸€æ•°æ®é€šè·¯ä¸Šï¼ˆä½å®½éœ€åŒ¹é…ï¼‰
- æ˜¯å¦å…±äº«èµ„æºï¼ˆæ€»å¸¦å®½ã€æ€»é¢ç§¯ç­‰ï¼‰
- æ˜¯å¦å­˜åœ¨åŠŸèƒ½ä¾èµ–ï¼ˆä¸€ä¸ªç°‡çš„ä½¿èƒ½å½±å“å¦ä¸€ç°‡çš„é…ç½®ï¼‰

---

## è¾“å‡ºè¦æ±‚

**JSONæ ¼å¼**ï¼š

```json
{{
  "has_coupling": true,
  "cluster_analysis": [
    {{
      "cluster1": "ç°‡åç§°1",
      "cluster2": "ç°‡åç§°2",
      "relationship": "COUPLED | INDEPENDENT",
      "summary": "ä¸€å¥è¯è¯´æ˜è¿™ä¸¤ä¸ªç°‡çš„å…³ç³»"
    }}
  ],
  "couplings": [
    {{
      "param1": "æ¥è‡ªç°‡1çš„å‚æ•°å",
      "param2": "æ¥è‡ªç°‡2çš„å‚æ•°å",
      "param1_cluster": "ç°‡åç§°1",
      "param2_cluster": "ç°‡åç§°2",
      "type": "CROSS_CLUSTER_CONSTRAINT | CROSS_CLUSTER_CONDITIONAL | CROSS_CLUSTER_DERIVATION",
      "description": "æ¸…æ™°æè¿°è·¨ç°‡è€¦åˆå…³ç³»",
      "rule": "å½¢å¼åŒ–è§„åˆ™",
      "confidence": "high | medium | low",
      "reasoning": "ä¸ºä»€ä¹ˆè®¤ä¸ºå­˜åœ¨è·¨ç°‡è€¦åˆ"
    }}
  ]
}}

**æ³¨æ„**ï¼š
- åªè¾“å‡ºJSON
- å¦‚æœæ‰€æœ‰ç°‡éƒ½ç‹¬ç«‹ï¼Œä¹Ÿè¦åœ¨ `cluster_analysis` ä¸­æ˜ç¡®æ ‡æ³¨
- ä¸è¦è‡†æµ‹è¿‡åº¦ï¼Œèšç„¦äºå®é™…å¯èƒ½å­˜åœ¨çš„è€¦åˆ

---

è¯·å¼€å§‹åˆ†æã€‚
"""
        return prompt
    
    def generate_prompt(self, pair: Dict) -> str:
        """æ ¹æ®ç±»å‹ç”Ÿæˆå¯¹åº”çš„æç¤ºè¯"""
        if pair['type'] == 'INTER_FILE':
            return self.generate_inter_file_prompt(pair)
        elif pair['type'] == 'INTRA_FILE_MULTI_CLUSTER':
            return self.generate_intra_file_prompt(pair)
        else:
            raise ValueError(f"æœªçŸ¥çš„åˆ†æç±»å‹: {pair['type']}")
    
    def call_llm(self, prompt: str, model: str = "claude-3-5-sonnet-20241022"):
        """
        è°ƒç”¨LLM API
        TODO: å®ç°å®é™…çš„APIè°ƒç”¨é€»è¾‘
        """
        # è¿™é‡Œéœ€è¦æ ¹æ®æ‚¨ä½¿ç”¨çš„LLM APIè¿›è¡Œå®ç°
        # ç¤ºä¾‹æ¡†æ¶ï¼š
        # import anthropic
        # client = anthropic.Anthropic(api_key="your-api-key")
        # response = client.messages.create(
        #     model=model,
        #     max_tokens=2048,
        #     messages=[{"role": "user", "content": prompt}]
        # )
        # return json.loads(response.content[0].text)
        pass

    def analyze_all(self, max_pairs: int = None):
        """åˆ†ææ‰€æœ‰æ–‡ä»¶å¯¹"""
        
        print("="*70)
        print("ğŸ¤– Step 4: LLM åˆ†æ")
        print("="*70)
        
        pairs_to_analyze = self.file_pairs[:max_pairs] if max_pairs else self.file_pairs
        
        # ç»Ÿè®¡ä»»åŠ¡ç±»å‹
        inter_file_pairs = [p for p in pairs_to_analyze if p['type'] == 'INTER_FILE']
        intra_file_pairs = [p for p in pairs_to_analyze if p['type'] == 'INTRA_FILE_MULTI_CLUSTER']
        
        print(f"å‡†å¤‡åˆ†æ {len(pairs_to_analyze)} ä¸ªä»»åŠ¡:")
        print(f"  - è·¨æ–‡ä»¶åˆ†æ: {len(inter_file_pairs)}")
        print(f"  - å•æ–‡ä»¶å¤šç°‡åˆ†æ: {len(intra_file_pairs)}\n")
        
        results = []
        
        for i, pair in enumerate(pairs_to_analyze, 1):
            if pair['type'] == 'INTER_FILE':
                caller_name = Path(pair['caller_file']).name
                callee_name = Path(pair['callee_file']).name
                print(f"[{i}/{len(pairs_to_analyze)}] è·¨æ–‡ä»¶: {caller_name} â†’ {callee_name}", end=' ')
            else:
                file_name = Path(pair['file']).name
                cluster_info = f"{len(pair['clusters'])} ç°‡"
                print(f"[{i}/{len(pairs_to_analyze)}] å•æ–‡ä»¶å¤šç°‡: {file_name} ({cluster_info})", end=' ')
            
            prompt = self.generate_prompt(pair)
            
            try:
                analysis = self.call_llm(prompt)
                
                if analysis and analysis.get('has_coupling'):
                    coupling_count = len(analysis.get('couplings', []))
                    print(f"âœ… å‘ç° {coupling_count} ä¸ªè€¦åˆ")
                    
                    results.append({
                        'task': pair,
                        'analysis': analysis
                    })
                else:
                    print("â– æ— è€¦åˆ")
            
            except Exception as e:
                print(f"âŒ é”™è¯¯: {e}")
        
        print(f"\nâœ… LLMåˆ†æå®Œæˆï¼Œå…± {len(results)} ä¸ªä»»åŠ¡å‘ç°è€¦åˆ\n")
        
        return results
    
class CouplingExtractor:
    """ä»LLMç»“æœæå–è€¦åˆå…³ç³»"""
    def __init__(self, llm_results: List[Dict]):
        self.llm_results = llm_results
    
    def extract(self) -> List[Dict]:
        """æå–æ‰€æœ‰è€¦åˆå…³ç³»"""
        
        print("="*70)
        print("ğŸ“Š Step 5: æå–è€¦åˆå…³ç³»")
        print("="*70)
        
        all_couplings = []
        
        for result in self.llm_results:
            task = result['task']
            analysis = result['analysis']
            
            if task['type'] == 'INTER_FILE':
                # è·¨æ–‡ä»¶è€¦åˆ
                for c in analysis.get('couplings', []):
                    coupling = {
                        'scope': 'INTER_FILE',
                        'param1': c.get('caller_param'),
                        'param2': c.get('callee_param'),
                        'cluster1': task.get('caller_clusters', []),
                        'cluster2': task.get('callee_clusters', []),
                        'type': c.get('type'),
                        'description': c.get('description'),
                        'rule': c.get('rule'),
                        'confidence': c.get('confidence', 'medium'),
                        'evidence': {
                            'caller_file': task['caller_file'],
                            'callee_file': task['callee_file'],
                            'module': task['module'],
                            'instance': task['instance']
                        }
                    }
                    all_couplings.append(coupling)
            
            elif task['type'] == 'INTRA_FILE_MULTI_CLUSTER':
                # å•æ–‡ä»¶å¤šç°‡è€¦åˆ
                for c in analysis.get('couplings', []):
                    coupling = {
                        'scope': 'INTRA_FILE_CROSS_CLUSTER',
                        'param1': c.get('param1'),
                        'param2': c.get('param2'),
                        'cluster1': c.get('param1_cluster'),
                        'cluster2': c.get('param2_cluster'),
                        'type': c.get('type'),
                        'description': c.get('description'),
                        'rule': c.get('rule'),
                        'confidence': c.get('confidence', 'medium'),
                        'evidence': {
                            'file': task['file'],
                            'role': task['role'],
                            'other_file': task['other_file'],
                            'module': task['module'],
                            'instance': task['instance']
                        }
                    }
                    all_couplings.append(coupling)
        
        print(f"âœ… æå–åˆ° {len(all_couplings)} æ¡è€¦åˆå…³ç³»\n")
        
        return all_couplings

    def build_graph(self, couplings: List[Dict]) -> nx.DiGraph:
        """æ„å»ºè€¦åˆå…³ç³»å›¾"""
        
        G = nx.DiGraph()
        
        for c in couplings:
            p1 = c['param1']
            p2 = c['param2']
            
            if p1 and p2:
                G.add_edge(
                    p1, p2,
                    type=c['type'],
                    description=c['description'],
                    rule=c['rule'],
                    confidence=c['confidence']
                )
        
        return G

    def generate_summary(self, couplings: List[Dict]) -> Dict:
        """ç”Ÿæˆç»Ÿè®¡æ‘˜è¦"""
        
        type_counts = defaultdict(int)
        scope_counts = defaultdict(int)
        conf_counts = defaultdict(int)
        
        for c in couplings:
            type_counts[c['type']] += 1
            scope_counts[c['scope']] += 1
            conf_counts[c['confidence']] += 1
        
        return {
            'total_couplings': len(couplings),
            'unique_params': len(set([c['param1'] for c in couplings] + [c['param2'] for c in couplings])),
            'by_type': dict(type_counts),
            'by_scope': dict(scope_counts),
            'by_confidence': dict(conf_counts),
            'inter_file_couplings': sum(1 for c in couplings if c['scope'] == 'INTER_FILE'),
            'cross_cluster_couplings': sum(1 for c in couplings if c['scope'] == 'INTRA_FILE_CROSS_CLUSTER')
        }
    
def main(): 
    """ä¸»æµç¨‹"""
    print("\n" + "="*70)
    print("ğŸš€ å‚æ•°è€¦åˆå…³ç³»åˆ†æç³»ç»Ÿ")
    print("="*70 + "\n")

    # é…ç½®è·¯å¾„
    dependency_json = Path("dependency_analysis.json")
    clusters_json = Path("clusters.json")
    params_file = Path("cfg_params/fdae_top_template.in_pdt")
    driver_root = Path("driver/")

    # Step 1: åŠ è½½æ•°æ®ï¼ˆåŒ…æ‹¬å®Œæ•´å‚æ•°é…ç½®ï¼‰
    loader = DependencyLoader(dependency_json, clusters_json, params_file)
    loader.load()

    # Step 2: å­—ç¬¦ä¸²åŒ¹é…ï¼ˆé™„åŠ å®Œæ•´å‚æ•°ä¿¡æ¯ï¼‰
    matcher = StringMatcher(driver_root, loader.clusters, loader.params_info)
    candidates = matcher.scan_all()

    # ä¿å­˜candidates
    with open('candidates.json', 'w', encoding='utf-8') as f:
        json.dump(candidates, f, indent=2, ensure_ascii=False)
    print("ğŸ’¾ å·²ä¿å­˜: candidates.json\n")

    # Step 3: æ„å»ºæ–‡ä»¶å¯¹ï¼ˆæ”¯æŒå•ç«¯å¤šç°‡å’ŒåŒç«¯åˆ†æï¼‰
    pair_builder = FilePairBuilder(loader.dependency_data, candidates)
    file_pairs = pair_builder.build_pairs()

    # ä¿å­˜file_pairs
    with open('file_pairs.json', 'w', encoding='utf-8') as f:
        json.dump(file_pairs, f, indent=2, ensure_ascii=False)
    print("ğŸ’¾ å·²ä¿å­˜: file_pairs.json\n")

    # Step 4: LLMåˆ†æï¼ˆä¼ å…¥ç°‡å®šä¹‰ç”¨äºåˆ†ç»„æ˜¾ç¤ºï¼‰
    llm_analyzer = LLMCouplingAnalyzer(file_pairs, loader.clusters)
    # æµ‹è¯•ï¼šåªåˆ†æå‰5å¯¹
    # llm_results = llm_analyzer.analyze_all(max_pairs=5)
    # å®Œæ•´åˆ†æï¼š
    llm_results = llm_analyzer.analyze_all()

    # ä¿å­˜LLMç»“æœ
    with open('coupling_analysis_results.json', 'w', encoding='utf-8') as f:
        json.dump(llm_results, f, indent=2, ensure_ascii=False)
    print("ğŸ’¾ å·²ä¿å­˜: coupling_analysis_results.json\n")

    # Step 5: æå–è€¦åˆå…³ç³»
    extractor = CouplingExtractor(llm_results)
    couplings = extractor.extract()

    # ä¿å­˜è€¦åˆå…³ç³»
    with open('extracted_param_couplings.json', 'w', encoding='utf-8') as f:
        json.dump(couplings, f, indent=2, ensure_ascii=False)
    print("ğŸ’¾ å·²ä¿å­˜: extracted_param_couplings.json\n")

    # ç”Ÿæˆæ‘˜è¦
    summary = extractor.generate_summary(couplings)
    with open('param_couplings_summary.json', 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    print("ğŸ’¾ å·²ä¿å­˜: param_couplings_summary.json\n")

    # æ„å»ºå›¾
    graph = extractor.build_graph(couplings)
    nx.write_gexf(graph, 'coupling_graph.gexf')
    print("ğŸ’¾ å·²ä¿å­˜: coupling_graph.gexf\n")

    # æ‰“å°æœ€ç»ˆç»Ÿè®¡
    print("="*70)
    print("ğŸ“Š æœ€ç»ˆç»Ÿè®¡")
    print("="*70)
    print(f"æ€»è€¦åˆæ•°: {summary['total_couplings']}")
    print(f"æ¶‰åŠå‚æ•°: {summary['unique_params']}")
    print(f"\næŒ‰èŒƒå›´:")
    print(f"  - è·¨æ–‡ä»¶è€¦åˆ: {summary['inter_file_couplings']}")
    print(f"  - è·¨ç°‡è€¦åˆ (å•æ–‡ä»¶): {summary['cross_cluster_couplings']}")
    print(f"\næŒ‰ç±»å‹:")
    for t, count in summary['by_type'].items():
        print(f"  - {t}: {count}")
    print(f"\næŒ‰ç½®ä¿¡åº¦:")
    for conf, count in summary['by_confidence'].items():
        print(f"  - {conf}: {count}")
    print("\n" + "="*70)
    print("âœ… åˆ†æå®Œæˆï¼")
    print("="*70)

if __name__ == '__main__': 
    main()